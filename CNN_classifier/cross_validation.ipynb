{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from modules import torch_classes\n",
    "from torchvision import transforms, utils\n",
    "from modules.grad_cam import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set\n",
    "labels_path = '../DATA/train_labels.pkl'\n",
    "data_path = 'train.dat'\n",
    "data_dims = (8269,10404)\n",
    "genes_path = 'train.csv'\n",
    "\n",
    "#Data and labels\n",
    "train = torch_classes.TumorDataset(labels_path,data_path,data_dims,genes_path,transform = transforms.Compose([torch_classes.ToImage(),torch_classes.ToTensor()]))\n",
    "\n",
    "#Test set\n",
    "labels_path = '../DATA/test_labels.pkl'\n",
    "data_path = 'test.dat'\n",
    "data_dims = (2085,10404)\n",
    "genes_path = 'test.csv'\n",
    "\n",
    "test = torch_classes.TumorDataset(labels_path,data_path,data_dims,genes_path,transform = transforms.Compose([torch_classes.ToImage(),torch_classes.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 75\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn stratified kfold splitter, iterate over the folds to run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x7f1622e5eb88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)\n",
    "skf.split(np.zeros(len(train.int_labels)), train.int_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV to find optimal number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0 loss is: 3.526394989167268\n",
      "Epoch 0 Batch 1 loss is: 3.734350669301763\n",
      "Epoch 0 Batch 2 loss is: 3.058990553101805\n",
      "Epoch 0 Batch 3 loss is: 2.8045508597411852\n",
      "Epoch 0 Batch 4 loss is: 2.6530753433712113\n",
      "Epoch 0 Batch 5 loss is: 2.591426679875444\n",
      "Epoch 0 Batch 6 loss is: 2.9702986891943866\n",
      "Epoch 0 Batch 7 loss is: 2.0337783129479874\n",
      "Epoch 0 Batch 8 loss is: 2.2438971160183843\n",
      "Epoch 0 Batch 9 loss is: 2.282969901815195\n",
      "Epoch 0 Batch 10 loss is: 2.1202171558270915\n",
      "Epoch 0 Batch 11 loss is: 2.2085308789001052\n",
      "Epoch 0 Batch 12 loss is: 2.037771703134282\n",
      "Epoch 0 Batch 13 loss is: 1.9373709842477929\n",
      "Epoch 0 Batch 14 loss is: 1.7592555547695437\n",
      "Epoch 0 Batch 15 loss is: 1.5762485790674152\n",
      "Epoch 0 Batch 16 loss is: 1.5301700636922906\n",
      "Epoch 0 Batch 17 loss is: 1.6216274740300602\n",
      "Epoch 0 Batch 18 loss is: 1.5379127275408697\n",
      "Epoch 0 Batch 19 loss is: 1.4458990303518962\n",
      "Epoch 0 Batch 20 loss is: 1.4225203783424403\n",
      "Epoch 0 Batch 21 loss is: 1.4084797070876958\n",
      "Epoch 0 Batch 22 loss is: 1.415934206015257\n",
      "Epoch 0 Batch 23 loss is: 1.129122796148706\n",
      "Epoch 0 Batch 24 loss is: 1.0752892284910338\n",
      "Epoch 0 Batch 25 loss is: 1.0435056568755647\n",
      "Epoch 0 Batch 26 loss is: 1.2987887295659537\n",
      "Epoch 0 Batch 27 loss is: 1.0002705494643003\n",
      "Epoch 0 Batch 28 loss is: 0.9433545120625124\n",
      "Epoch 0 Batch 29 loss is: 0.9066438990411851\n",
      "Epoch 0 Batch 30 loss is: 0.8556287122439774\n",
      "Epoch 0 Batch 31 loss is: 1.0783420795856409\n",
      "Epoch 0 Batch 32 loss is: 0.9371463227917849\n",
      "Epoch 0 Batch 33 loss is: 0.790375497618713\n",
      "Epoch 0 Batch 34 loss is: 0.6488423365730482\n",
      "Epoch 0 Batch 35 loss is: 0.7307471027858429\n",
      "Epoch 0 Batch 36 loss is: 0.7808061895867345\n",
      "Epoch 0 Batch 37 loss is: 0.7672854229731758\n",
      "Epoch 0 Batch 38 loss is: 0.6081430873265132\n",
      "Epoch 0 Batch 39 loss is: 0.7113569185930144\n",
      "Epoch 0 Batch 40 loss is: 0.7115727655750429\n",
      "Epoch 0 Batch 41 loss is: 0.5660864860591241\n",
      "Epoch 0 Batch 42 loss is: 0.5734411899637306\n",
      "Epoch 0 Batch 43 loss is: 0.5843338832825412\n",
      "Epoch 0 Batch 44 loss is: 0.8710816794502892\n",
      "Epoch 0 Batch 45 loss is: 0.5132048414101591\n",
      "Epoch 0 Batch 46 loss is: 0.4447129185118419\n",
      "Epoch 0 Batch 47 loss is: 0.9583020376189747\n",
      "Epoch 0 Batch 48 loss is: 0.43775964973626\n",
      "Epoch 0 Batch 49 loss is: 0.6545865356590178\n",
      "Epoch 0 Batch 50 loss is: 0.24774876402984758\n",
      "Epoch 0 Batch 51 loss is: 0.561565838851563\n",
      "Epoch 0 Batch 52 loss is: 0.49168741705961644\n",
      "Epoch 0 Batch 53 loss is: 0.5722525051260893\n",
      "Epoch 0 Batch 54 loss is: 0.5166794061570563\n",
      "Epoch 0 Batch 55 loss is: 0.4220424815052997\n",
      "Epoch 0 Batch 56 loss is: 0.5306217253295112\n",
      "Epoch 0 Batch 57 loss is: 0.42296757125819245\n",
      "Epoch 0 Batch 58 loss is: 0.40415899161649116\n",
      "Epoch 0 Batch 59 loss is: 0.489545039837548\n",
      "Epoch 0 Batch 60 loss is: 0.34092554971449723\n",
      "Epoch 0 Batch 61 loss is: 0.4549041850588081\n",
      "Epoch 0 Batch 62 loss is: 0.20707385008901083\n",
      "Epoch 0 Batch 63 loss is: 0.4638255977227115\n",
      "Epoch 0 Batch 64 loss is: 0.4068967581182862\n",
      "Epoch 0 Batch 65 loss is: 0.37760522854563416\n",
      "Epoch 0 Batch 66 loss is: 0.4364379669369784\n",
      "Epoch 0 Batch 67 loss is: 0.5732551662055605\n",
      "Epoch 0 Batch 68 loss is: 0.4312037683979461\n",
      "Epoch 0 Batch 69 loss is: 0.403043283912685\n",
      "Epoch 0 Batch 70 loss is: 0.2957787635968318\n",
      "Epoch 0 Batch 71 loss is: 0.5198227814692887\n",
      "Epoch 0 Batch 72 loss is: 0.3057005489036639\n",
      "Epoch 0 Batch 73 loss is: 0.6775524328763741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|â–Ž         | 1/30 [04:39<2:14:53, 279.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 0 trn loss is: 1.1094013271466967 val loss is: 0.3838830445016632\n",
      "Epoch 1 Batch 0 loss is: 0.2340885143330947\n",
      "Epoch 1 Batch 1 loss is: 0.2772651324737858\n",
      "Epoch 1 Batch 2 loss is: 0.34755187386783004\n",
      "Epoch 1 Batch 3 loss is: 0.20636125881134404\n",
      "Epoch 1 Batch 4 loss is: 0.31229896083467046\n",
      "Epoch 1 Batch 5 loss is: 0.21393649523737898\n",
      "Epoch 1 Batch 6 loss is: 0.22157254930137532\n",
      "Epoch 1 Batch 7 loss is: 0.3229338597527511\n",
      "Epoch 1 Batch 8 loss is: 0.17659077036290202\n",
      "Epoch 1 Batch 9 loss is: 0.1902776763903071\n",
      "Epoch 1 Batch 10 loss is: 0.14315736164162374\n",
      "Epoch 1 Batch 11 loss is: 0.3185886003954748\n",
      "Epoch 1 Batch 12 loss is: 0.20752832166916194\n",
      "Epoch 1 Batch 13 loss is: 0.16161590679751886\n",
      "Epoch 1 Batch 14 loss is: 0.22207666287020839\n",
      "Epoch 1 Batch 15 loss is: 0.30388714737887457\n",
      "Epoch 1 Batch 16 loss is: 0.21956523114472565\n",
      "Epoch 1 Batch 17 loss is: 0.2825451141188534\n",
      "Epoch 1 Batch 18 loss is: 0.12416731852666267\n",
      "Epoch 1 Batch 19 loss is: 0.19800135202662666\n",
      "Epoch 1 Batch 20 loss is: 0.17287859306995482\n",
      "Epoch 1 Batch 21 loss is: 0.3463047712078272\n",
      "Epoch 1 Batch 22 loss is: 0.2834828224993011\n",
      "Epoch 1 Batch 23 loss is: 0.13861596387469563\n",
      "Epoch 1 Batch 24 loss is: 0.20906518071763713\n",
      "Epoch 1 Batch 25 loss is: 0.19673620739696437\n",
      "Epoch 1 Batch 26 loss is: 0.1560552077362962\n",
      "Epoch 1 Batch 27 loss is: 0.09093366061804958\n",
      "Epoch 1 Batch 28 loss is: 0.2077407816981142\n",
      "Epoch 1 Batch 29 loss is: 0.1638466819689132\n",
      "Epoch 1 Batch 30 loss is: 0.14380123206690767\n",
      "Epoch 1 Batch 31 loss is: 0.14910517125212072\n",
      "Epoch 1 Batch 32 loss is: 0.22637025996346913\n",
      "Epoch 1 Batch 33 loss is: 0.19953040874207564\n",
      "Epoch 1 Batch 34 loss is: 0.37280780850391904\n",
      "Epoch 1 Batch 35 loss is: 0.25208318653919026\n",
      "Epoch 1 Batch 36 loss is: 0.1870787583968919\n",
      "Epoch 1 Batch 37 loss is: 0.15439527262882885\n",
      "Epoch 1 Batch 38 loss is: 0.15758761625773013\n",
      "Epoch 1 Batch 39 loss is: 0.16442315595989318\n",
      "Epoch 1 Batch 40 loss is: 0.17196953762794842\n",
      "Epoch 1 Batch 41 loss is: 0.15934673740140787\n",
      "Epoch 1 Batch 42 loss is: 0.21014586885161185\n",
      "Epoch 1 Batch 43 loss is: 0.1897408120494031\n",
      "Epoch 1 Batch 44 loss is: 0.2267129875149715\n",
      "Epoch 1 Batch 45 loss is: 0.26817854554514065\n",
      "Epoch 1 Batch 46 loss is: 0.12289381888279899\n",
      "Epoch 1 Batch 47 loss is: 0.1987939754761862\n",
      "Epoch 1 Batch 48 loss is: 0.13949091726779328\n",
      "Epoch 1 Batch 49 loss is: 0.2741909975067778\n",
      "Epoch 1 Batch 50 loss is: 0.13932238507865963\n",
      "Epoch 1 Batch 51 loss is: 0.16038780627144578\n",
      "Epoch 1 Batch 52 loss is: 0.20530197094697858\n",
      "Epoch 1 Batch 53 loss is: 0.29078691059899653\n",
      "Epoch 1 Batch 54 loss is: 0.1743280838253448\n",
      "Epoch 1 Batch 55 loss is: 0.19554317312786\n",
      "Epoch 1 Batch 56 loss is: 0.09185032252811696\n",
      "Epoch 1 Batch 57 loss is: 0.1902397027045565\n",
      "Epoch 1 Batch 58 loss is: 0.1912494140619968\n",
      "Epoch 1 Batch 59 loss is: 0.1693866129884963\n",
      "Epoch 1 Batch 60 loss is: 0.1380555218777822\n",
      "Epoch 1 Batch 61 loss is: 0.1315291121277184\n",
      "Epoch 1 Batch 62 loss is: 0.15273743483787428\n",
      "Epoch 1 Batch 63 loss is: 0.1363074561739061\n",
      "Epoch 1 Batch 64 loss is: 0.17114567545055126\n",
      "Epoch 1 Batch 65 loss is: 0.07713022413456262\n",
      "Epoch 1 Batch 66 loss is: 0.22572381047346218\n",
      "Epoch 1 Batch 67 loss is: 0.1935330687871531\n",
      "Epoch 1 Batch 68 loss is: 0.20189358069920957\n",
      "Epoch 1 Batch 69 loss is: 0.11849623753213753\n",
      "Epoch 1 Batch 70 loss is: 0.16457366390614306\n",
      "Epoch 1 Batch 71 loss is: 0.15084573594294592\n",
      "Epoch 1 Batch 72 loss is: 0.2791381371023616\n",
      "Epoch 1 Batch 73 loss is: 0.24683746447666924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|â–‹         | 2/30 [09:16<2:09:58, 278.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 1 trn loss is: 0.20019811557855294 val loss is: 0.27030171395854297\n",
      "Epoch 2 Batch 0 loss is: 0.110746220090146\n",
      "Epoch 2 Batch 1 loss is: 0.08003607990583192\n",
      "Epoch 2 Batch 2 loss is: 0.06735338395587191\n",
      "Epoch 2 Batch 3 loss is: 0.06783151995877601\n",
      "Epoch 2 Batch 4 loss is: 0.08524570066722308\n",
      "Epoch 2 Batch 5 loss is: 0.07025220408574086\n",
      "Epoch 2 Batch 6 loss is: 0.14948928489323787\n",
      "Epoch 2 Batch 7 loss is: 0.09156333040719847\n",
      "Epoch 2 Batch 8 loss is: 0.057033819463421766\n",
      "Epoch 2 Batch 9 loss is: 0.09450885290295466\n",
      "Epoch 2 Batch 10 loss is: 0.08604039753154348\n",
      "Epoch 2 Batch 11 loss is: 0.0659681597964413\n",
      "Epoch 2 Batch 12 loss is: 0.05539656152792535\n",
      "Epoch 2 Batch 13 loss is: 0.06000104432629214\n",
      "Epoch 2 Batch 14 loss is: 0.09998444106680783\n",
      "Epoch 2 Batch 15 loss is: 0.06098862606239597\n",
      "Epoch 2 Batch 16 loss is: 0.0826831390214546\n",
      "Epoch 2 Batch 17 loss is: 0.0500740885036428\n",
      "Epoch 2 Batch 18 loss is: 0.04258995072321738\n",
      "Epoch 2 Batch 19 loss is: 0.04051752872347284\n",
      "Epoch 2 Batch 20 loss is: 0.08464004478056984\n",
      "Epoch 2 Batch 21 loss is: 0.10894953202256294\n",
      "Epoch 2 Batch 22 loss is: 0.07198133952421802\n",
      "Epoch 2 Batch 23 loss is: 0.0814775452012056\n",
      "Epoch 2 Batch 24 loss is: 0.054879058411959544\n",
      "Epoch 2 Batch 25 loss is: 0.04183189178415159\n",
      "Epoch 2 Batch 26 loss is: 0.04530117937090328\n",
      "Epoch 2 Batch 27 loss is: 0.05331954475777003\n",
      "Epoch 2 Batch 28 loss is: 0.035045688744221856\n",
      "Epoch 2 Batch 29 loss is: 0.050387240757198\n",
      "Epoch 2 Batch 30 loss is: 0.055194514775850745\n",
      "Epoch 2 Batch 31 loss is: 0.04345521599601201\n",
      "Epoch 2 Batch 32 loss is: 0.054200361227212376\n",
      "Epoch 2 Batch 33 loss is: 0.05772266983158366\n",
      "Epoch 2 Batch 34 loss is: 0.06948662966059313\n",
      "Epoch 2 Batch 35 loss is: 0.0709969489208554\n",
      "Epoch 2 Batch 36 loss is: 0.05088885599060473\n",
      "Epoch 2 Batch 37 loss is: 0.03644306025641288\n",
      "Epoch 2 Batch 38 loss is: 0.04424817429318712\n",
      "Epoch 2 Batch 39 loss is: 0.05810145592152958\n",
      "Epoch 2 Batch 40 loss is: 0.06598046764826951\n",
      "Epoch 2 Batch 41 loss is: 0.05549547292106531\n",
      "Epoch 2 Batch 42 loss is: 0.040991033460053256\n",
      "Epoch 2 Batch 43 loss is: 0.06099326181336391\n",
      "Epoch 2 Batch 44 loss is: 0.026110673538032404\n",
      "Epoch 2 Batch 45 loss is: 0.036747498105116856\n",
      "Epoch 2 Batch 46 loss is: 0.07086859187442203\n",
      "Epoch 2 Batch 47 loss is: 0.08544714746878776\n",
      "Epoch 2 Batch 48 loss is: 0.04418549471131488\n",
      "Epoch 2 Batch 49 loss is: 0.02516958707392861\n",
      "Epoch 2 Batch 50 loss is: 0.04827348605355292\n",
      "Epoch 2 Batch 51 loss is: 0.0640359569006651\n",
      "Epoch 2 Batch 52 loss is: 0.04594307189455981\n",
      "Epoch 2 Batch 53 loss is: 0.1342491740433479\n",
      "Epoch 2 Batch 54 loss is: 0.04236441652209775\n",
      "Epoch 2 Batch 55 loss is: 0.022314558127947692\n",
      "Epoch 2 Batch 56 loss is: 0.020293943928547796\n",
      "Epoch 2 Batch 57 loss is: 0.06736358750666448\n",
      "Epoch 2 Batch 58 loss is: 0.07534687044514611\n",
      "Epoch 2 Batch 59 loss is: 0.07597341046476289\n",
      "Epoch 2 Batch 60 loss is: 0.023989101905901825\n",
      "Epoch 2 Batch 61 loss is: 0.043947060877134354\n",
      "Epoch 2 Batch 62 loss is: 0.01911267879771742\n",
      "Epoch 2 Batch 63 loss is: 0.04086253987251587\n",
      "Epoch 2 Batch 64 loss is: 0.06453069060563761\n",
      "Epoch 2 Batch 65 loss is: 0.033989463355060125\n",
      "Epoch 2 Batch 66 loss is: 0.10011840090626604\n",
      "Epoch 2 Batch 67 loss is: 0.052426091389553425\n",
      "Epoch 2 Batch 68 loss is: 0.07443019746218486\n",
      "Epoch 2 Batch 69 loss is: 0.13111543509805484\n",
      "Epoch 2 Batch 70 loss is: 0.08618516099956128\n",
      "Epoch 2 Batch 71 loss is: 0.03737028180819634\n",
      "Epoch 2 Batch 72 loss is: 0.03741105959191478\n",
      "Epoch 2 Batch 73 loss is: 0.019397384487458213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|â–ˆ         | 3/30 [13:52<2:05:05, 277.96s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 2 trn loss is: 0.06162011537163516 val loss is: 0.22012910351070514\n",
      "Epoch 3 Batch 0 loss is: 0.032005120226269286\n",
      "Epoch 3 Batch 1 loss is: 0.030444749836491147\n",
      "Epoch 3 Batch 2 loss is: 0.028469887976702496\n",
      "Epoch 3 Batch 3 loss is: 0.03611809889731373\n",
      "Epoch 3 Batch 4 loss is: 0.03293422494593008\n",
      "Epoch 3 Batch 5 loss is: 0.025212381076764375\n",
      "Epoch 3 Batch 6 loss is: 0.0360155128351954\n",
      "Epoch 3 Batch 7 loss is: 0.02113452084556947\n",
      "Epoch 3 Batch 8 loss is: 0.0332236319341042\n",
      "Epoch 3 Batch 9 loss is: 0.028608796841826004\n",
      "Epoch 3 Batch 10 loss is: 0.03580494513520689\n",
      "Epoch 3 Batch 11 loss is: 0.023940384467573933\n",
      "Epoch 3 Batch 12 loss is: 0.019260425178120228\n",
      "Epoch 3 Batch 13 loss is: 0.035133009835435845\n",
      "Epoch 3 Batch 14 loss is: 0.027501660890966104\n",
      "Epoch 3 Batch 15 loss is: 0.01671551323840164\n",
      "Epoch 3 Batch 16 loss is: 0.029221111547352324\n",
      "Epoch 3 Batch 17 loss is: 0.02002017693789463\n",
      "Epoch 3 Batch 18 loss is: 0.016920592017687176\n",
      "Epoch 3 Batch 19 loss is: 0.023202232247072418\n",
      "Epoch 3 Batch 20 loss is: 0.025530860952345916\n",
      "Epoch 3 Batch 21 loss is: 0.01623568210993304\n",
      "Epoch 3 Batch 22 loss is: 0.02523473535410814\n",
      "Epoch 3 Batch 23 loss is: 0.00748794204715054\n",
      "Epoch 3 Batch 24 loss is: 0.017337265474193668\n",
      "Epoch 3 Batch 25 loss is: 0.025231249533085306\n",
      "Epoch 3 Batch 26 loss is: 0.01749480643700045\n",
      "Epoch 3 Batch 27 loss is: 0.024480347444222658\n",
      "Epoch 3 Batch 28 loss is: 0.022274227852551908\n",
      "Epoch 3 Batch 29 loss is: 0.017233687899975366\n",
      "Epoch 3 Batch 30 loss is: 0.02249264865045473\n",
      "Epoch 3 Batch 31 loss is: 0.021554105214753903\n",
      "Epoch 3 Batch 32 loss is: 0.01794646845317601\n",
      "Epoch 3 Batch 33 loss is: 0.031326378911561197\n",
      "Epoch 3 Batch 34 loss is: 0.010671241363175668\n",
      "Epoch 3 Batch 35 loss is: 0.013496973545237729\n",
      "Epoch 3 Batch 36 loss is: 0.009516205356421519\n",
      "Epoch 3 Batch 37 loss is: 0.01006638388964639\n",
      "Epoch 3 Batch 38 loss is: 0.013315652286487976\n",
      "Epoch 3 Batch 39 loss is: 0.012010551169687992\n",
      "Epoch 3 Batch 40 loss is: 0.042025986086030497\n",
      "Epoch 3 Batch 41 loss is: 0.039599256549419766\n",
      "Epoch 3 Batch 42 loss is: 0.011458941192463866\n",
      "Epoch 3 Batch 43 loss is: 0.017259096468928245\n",
      "Epoch 3 Batch 44 loss is: 0.00791230179070529\n",
      "Epoch 3 Batch 45 loss is: 0.019285553599728923\n",
      "Epoch 3 Batch 46 loss is: 0.0095019891847238\n",
      "Epoch 3 Batch 47 loss is: 0.013719297291082016\n",
      "Epoch 3 Batch 48 loss is: 0.011050332444423307\n",
      "Epoch 3 Batch 49 loss is: 0.0157950803505202\n",
      "Epoch 3 Batch 50 loss is: 0.012916589095925156\n",
      "Epoch 3 Batch 51 loss is: 0.012178222108407985\n",
      "Epoch 3 Batch 52 loss is: 0.02162714100977233\n",
      "Epoch 3 Batch 53 loss is: 0.019708208869391904\n",
      "Epoch 3 Batch 54 loss is: 0.008782019144157082\n",
      "Epoch 3 Batch 55 loss is: 0.015128915569753213\n",
      "Epoch 3 Batch 56 loss is: 0.015613208011873034\n",
      "Epoch 3 Batch 57 loss is: 0.01728097695443458\n",
      "Epoch 3 Batch 58 loss is: 0.03147358205281094\n",
      "Epoch 3 Batch 59 loss is: 0.015143679003404967\n",
      "Epoch 3 Batch 60 loss is: 0.013972569404364433\n",
      "Epoch 3 Batch 61 loss is: 0.009007395187212724\n",
      "Epoch 3 Batch 62 loss is: 0.011088384569283653\n",
      "Epoch 3 Batch 63 loss is: 0.0126684579451319\n",
      "Epoch 3 Batch 64 loss is: 0.019034984172831823\n",
      "Epoch 3 Batch 65 loss is: 0.018531378729113836\n",
      "Epoch 3 Batch 66 loss is: 0.020320094252286404\n",
      "Epoch 3 Batch 67 loss is: 0.026149265766406204\n",
      "Epoch 3 Batch 68 loss is: 0.010176484868524571\n",
      "Epoch 3 Batch 69 loss is: 0.020294831716575614\n",
      "Epoch 3 Batch 70 loss is: 0.016219418134627853\n",
      "Epoch 3 Batch 71 loss is: 0.026674704564860114\n",
      "Epoch 3 Batch 72 loss is: 0.025278971344883806\n",
      "Epoch 3 Batch 73 loss is: 0.03174346081032223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 13%|â–ˆâ–Ž        | 4/30 [18:29<2:00:15, 277.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 3 trn loss is: 0.02069520498785716 val loss is: 0.2028622677513221\n",
      "Epoch 4 Batch 0 loss is: 0.013289743667583948\n",
      "Epoch 4 Batch 1 loss is: 0.015536424252992908\n",
      "Epoch 4 Batch 2 loss is: 0.019318240799764463\n",
      "Epoch 4 Batch 3 loss is: 0.008197325348680819\n",
      "Epoch 4 Batch 4 loss is: 0.01750242997240805\n",
      "Epoch 4 Batch 5 loss is: 0.016325094231767815\n",
      "Epoch 4 Batch 6 loss is: 0.010941045957473333\n",
      "Epoch 4 Batch 7 loss is: 0.0112498148544224\n",
      "Epoch 4 Batch 8 loss is: 0.007087786337849049\n",
      "Epoch 4 Batch 9 loss is: 0.012791673318595839\n",
      "Epoch 4 Batch 10 loss is: 0.0052627216865980845\n",
      "Epoch 4 Batch 11 loss is: 0.009775189525819043\n",
      "Epoch 4 Batch 12 loss is: 0.010037754749576718\n",
      "Epoch 4 Batch 13 loss is: 0.006541558147421789\n",
      "Epoch 4 Batch 14 loss is: 0.010168191094528692\n",
      "Epoch 4 Batch 15 loss is: 0.01050611408114431\n",
      "Epoch 4 Batch 16 loss is: 0.006320644658530969\n",
      "Epoch 4 Batch 17 loss is: 0.007166548606502517\n",
      "Epoch 4 Batch 18 loss is: 0.00916196639712659\n",
      "Epoch 4 Batch 19 loss is: 0.010157340866401132\n",
      "Epoch 4 Batch 20 loss is: 0.0053271370717035416\n",
      "Epoch 4 Batch 21 loss is: 0.0067484927494158835\n",
      "Epoch 4 Batch 22 loss is: 0.009816389398887005\n",
      "Epoch 4 Batch 23 loss is: 0.008007317427327057\n",
      "Epoch 4 Batch 24 loss is: 0.007815622790472822\n",
      "Epoch 4 Batch 25 loss is: 0.009728433384093607\n",
      "Epoch 4 Batch 26 loss is: 0.013192573593870875\n",
      "Epoch 4 Batch 27 loss is: 0.009904532156946028\n",
      "Epoch 4 Batch 28 loss is: 0.00784730947903204\n",
      "Epoch 4 Batch 29 loss is: 0.011616143606186699\n",
      "Epoch 4 Batch 30 loss is: 0.013363041092273409\n",
      "Epoch 4 Batch 31 loss is: 0.009967601448034836\n",
      "Epoch 4 Batch 32 loss is: 0.010867077304236344\n",
      "Epoch 4 Batch 33 loss is: 0.009991852202501454\n",
      "Epoch 4 Batch 34 loss is: 0.009888456512740748\n",
      "Epoch 4 Batch 35 loss is: 0.0072735710385305906\n",
      "Epoch 4 Batch 36 loss is: 0.01889832723446928\n",
      "Epoch 4 Batch 37 loss is: 0.008894286673374446\n",
      "Epoch 4 Batch 38 loss is: 0.008195195654699082\n",
      "Epoch 4 Batch 39 loss is: 0.008602251769942414\n",
      "Epoch 4 Batch 40 loss is: 0.007703264808009867\n",
      "Epoch 4 Batch 41 loss is: 0.009993324109934048\n",
      "Epoch 4 Batch 42 loss is: 0.010236798447934537\n",
      "Epoch 4 Batch 43 loss is: 0.010189337524261136\n",
      "Epoch 4 Batch 44 loss is: 0.009167341079408805\n",
      "Epoch 4 Batch 45 loss is: 0.010424689979935486\n",
      "Epoch 4 Batch 46 loss is: 0.010562245082372369\n",
      "Epoch 4 Batch 47 loss is: 0.006133821638480974\n",
      "Epoch 4 Batch 48 loss is: 0.0054857598155609345\n",
      "Epoch 4 Batch 49 loss is: 0.007487985759704401\n",
      "Epoch 4 Batch 50 loss is: 0.007182699223801248\n",
      "Epoch 4 Batch 51 loss is: 0.006905345081851131\n",
      "Epoch 4 Batch 52 loss is: 0.00882773803098902\n",
      "Epoch 4 Batch 53 loss is: 0.004774702664823588\n",
      "Epoch 4 Batch 54 loss is: 0.012109893094849711\n",
      "Epoch 4 Batch 55 loss is: 0.007549866681985821\n",
      "Epoch 4 Batch 56 loss is: 0.006788579532291017\n",
      "Epoch 4 Batch 57 loss is: 0.007981687897708057\n",
      "Epoch 4 Batch 58 loss is: 0.005862021936499353\n",
      "Epoch 4 Batch 59 loss is: 0.009534191084488235\n",
      "Epoch 4 Batch 60 loss is: 0.00573915162867736\n",
      "Epoch 4 Batch 61 loss is: 0.005980619644974823\n",
      "Epoch 4 Batch 62 loss is: 0.006421155259527787\n",
      "Epoch 4 Batch 63 loss is: 0.004030640867976677\n",
      "Epoch 4 Batch 64 loss is: 0.010720339019118359\n",
      "Epoch 4 Batch 65 loss is: 0.009643711769008547\n",
      "Epoch 4 Batch 66 loss is: 0.00998267630592423\n",
      "Epoch 4 Batch 67 loss is: 0.005374983452090115\n",
      "Epoch 4 Batch 68 loss is: 0.010142215624295368\n",
      "Epoch 4 Batch 69 loss is: 0.007775215809032759\n",
      "Epoch 4 Batch 70 loss is: 0.005670042410730538\n",
      "Epoch 4 Batch 71 loss is: 0.00842509930851565\n",
      "Epoch 4 Batch 72 loss is: 0.008162673767386922\n",
      "Epoch 4 Batch 73 loss is: 0.005771729464773579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|â–ˆâ–‹        | 5/30 [23:07<1:55:39, 277.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 4 trn loss is: 0.00921655093176823 val loss is: 0.19745542689776543\n",
      "Epoch 5 Batch 0 loss is: 0.008924677928841464\n",
      "Epoch 5 Batch 1 loss is: 0.005279333881958869\n",
      "Epoch 5 Batch 2 loss is: 0.006184432363501446\n",
      "Epoch 5 Batch 3 loss is: 0.005139512895896559\n",
      "Epoch 5 Batch 4 loss is: 0.0076896708984055566\n",
      "Epoch 5 Batch 5 loss is: 0.005509764778694108\n",
      "Epoch 5 Batch 6 loss is: 0.007706219763222097\n",
      "Epoch 5 Batch 7 loss is: 0.0047785013438931285\n",
      "Epoch 5 Batch 8 loss is: 0.004181602391851958\n",
      "Epoch 5 Batch 9 loss is: 0.007955858205115901\n",
      "Epoch 5 Batch 10 loss is: 0.004547620313301858\n",
      "Epoch 5 Batch 11 loss is: 0.006682651547153533\n",
      "Epoch 5 Batch 12 loss is: 0.002640567958967163\n",
      "Epoch 5 Batch 13 loss is: 0.002532350706514753\n",
      "Epoch 5 Batch 14 loss is: 0.0056922264528238035\n",
      "Epoch 5 Batch 15 loss is: 0.006536873924101556\n",
      "Epoch 5 Batch 16 loss is: 0.0066917561486583566\n",
      "Epoch 5 Batch 17 loss is: 0.009236069626007091\n",
      "Epoch 5 Batch 18 loss is: 0.005326944591085491\n",
      "Epoch 5 Batch 19 loss is: 0.005063242458698293\n",
      "Epoch 5 Batch 20 loss is: 0.00516905890414345\n",
      "Epoch 5 Batch 21 loss is: 0.008380853913503034\n",
      "Epoch 5 Batch 22 loss is: 0.006250971044158753\n",
      "Epoch 5 Batch 23 loss is: 0.008095571823581242\n",
      "Epoch 5 Batch 24 loss is: 0.008199473770639449\n",
      "Epoch 5 Batch 25 loss is: 0.006993050107559569\n",
      "Epoch 5 Batch 26 loss is: 0.009232312639129235\n",
      "Epoch 5 Batch 27 loss is: 0.005884219494298198\n",
      "Epoch 5 Batch 28 loss is: 0.005332645695302366\n",
      "Epoch 5 Batch 29 loss is: 0.007333032379985257\n",
      "Epoch 5 Batch 30 loss is: 0.004809985050911791\n",
      "Epoch 5 Batch 31 loss is: 0.006535114087679249\n",
      "Epoch 5 Batch 32 loss is: 0.0034108792810325877\n",
      "Epoch 5 Batch 33 loss is: 0.005505322416878992\n",
      "Epoch 5 Batch 34 loss is: 0.005635713709118626\n",
      "Epoch 5 Batch 35 loss is: 0.008927081090082053\n",
      "Epoch 5 Batch 36 loss is: 0.006299270628652505\n",
      "Epoch 5 Batch 37 loss is: 0.00477741024565353\n",
      "Epoch 5 Batch 38 loss is: 0.003180876162263715\n",
      "Epoch 5 Batch 39 loss is: 0.004985910382617267\n",
      "Epoch 5 Batch 40 loss is: 0.0035554081342944245\n",
      "Epoch 5 Batch 41 loss is: 0.004603597720422954\n",
      "Epoch 5 Batch 42 loss is: 0.006317893078556409\n",
      "Epoch 5 Batch 43 loss is: 0.003665530170557858\n",
      "Epoch 5 Batch 44 loss is: 0.005326760759247868\n",
      "Epoch 5 Batch 45 loss is: 0.007404172010919533\n",
      "Epoch 5 Batch 46 loss is: 0.004800668262512768\n",
      "Epoch 5 Batch 47 loss is: 0.0033289188499929525\n",
      "Epoch 5 Batch 48 loss is: 0.004631976327058117\n",
      "Epoch 5 Batch 49 loss is: 0.0034341643614807064\n",
      "Epoch 5 Batch 50 loss is: 0.00621729955429006\n",
      "Epoch 5 Batch 51 loss is: 0.004604142966725722\n",
      "Epoch 5 Batch 52 loss is: 0.005571147742350935\n",
      "Epoch 5 Batch 53 loss is: 0.005872111060672888\n",
      "Epoch 5 Batch 54 loss is: 0.005692969072998911\n",
      "Epoch 5 Batch 55 loss is: 0.0040188574070943875\n",
      "Epoch 5 Batch 56 loss is: 0.0037811214079309253\n",
      "Epoch 5 Batch 57 loss is: 0.005960642339180306\n",
      "Epoch 5 Batch 58 loss is: 0.0031831499651498044\n",
      "Epoch 5 Batch 59 loss is: 0.0027693867357681157\n",
      "Epoch 5 Batch 60 loss is: 0.004031858349151219\n",
      "Epoch 5 Batch 61 loss is: 0.0036444040051257314\n",
      "Epoch 5 Batch 62 loss is: 0.004153239402511536\n",
      "Epoch 5 Batch 63 loss is: 0.006771630308201218\n",
      "Epoch 5 Batch 64 loss is: 0.0030126491481446984\n",
      "Epoch 5 Batch 65 loss is: 0.003660672884855387\n",
      "Epoch 5 Batch 66 loss is: 0.003946514953954636\n",
      "Epoch 5 Batch 67 loss is: 0.002947189511362609\n",
      "Epoch 5 Batch 68 loss is: 0.0035520158728243583\n",
      "Epoch 5 Batch 69 loss is: 0.004846580864040473\n",
      "Epoch 5 Batch 70 loss is: 0.0025909986119363993\n",
      "Epoch 5 Batch 71 loss is: 0.006034860657328274\n",
      "Epoch 5 Batch 72 loss is: 0.005648171493577768\n",
      "Epoch 5 Batch 73 loss is: 0.002141408406091827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|â–ˆâ–ˆ        | 6/30 [27:45<1:51:05, 277.74s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 5 trn loss is: 0.005337307343272916 val loss is: 0.21046600726875564\n",
      "Epoch 6 Batch 0 loss is: 0.0033113728234739643\n",
      "Epoch 6 Batch 1 loss is: 0.0064197320462226655\n",
      "Epoch 6 Batch 2 loss is: 0.005488292742012793\n",
      "Epoch 6 Batch 3 loss is: 0.0030768441161118667\n",
      "Epoch 6 Batch 4 loss is: 0.00451893263368575\n",
      "Epoch 6 Batch 5 loss is: 0.004120963738749642\n",
      "Epoch 6 Batch 6 loss is: 0.003501558699072594\n",
      "Epoch 6 Batch 7 loss is: 0.002483376358638812\n",
      "Epoch 6 Batch 8 loss is: 0.0023040547038710338\n",
      "Epoch 6 Batch 9 loss is: 0.004018492078641991\n",
      "Epoch 6 Batch 10 loss is: 0.006017590625546324\n",
      "Epoch 6 Batch 11 loss is: 0.003457276068015934\n",
      "Epoch 6 Batch 12 loss is: 0.004029580264920073\n",
      "Epoch 6 Batch 13 loss is: 0.0069543170558406565\n",
      "Epoch 6 Batch 14 loss is: 0.0038895353592610367\n",
      "Epoch 6 Batch 15 loss is: 0.00346337719000528\n",
      "Epoch 6 Batch 16 loss is: 0.005641966962670632\n",
      "Epoch 6 Batch 17 loss is: 0.0033238279179612106\n",
      "Epoch 6 Batch 18 loss is: 0.0036358019735268717\n",
      "Epoch 6 Batch 19 loss is: 0.0056479492523770014\n",
      "Epoch 6 Batch 20 loss is: 0.0033777888047641725\n",
      "Epoch 6 Batch 21 loss is: 0.0028489120647527955\n",
      "Epoch 6 Batch 22 loss is: 0.003663353192244424\n",
      "Epoch 6 Batch 23 loss is: 0.003117894065082645\n",
      "Epoch 6 Batch 24 loss is: 0.0020220910254929217\n",
      "Epoch 6 Batch 25 loss is: 0.00258950575455773\n",
      "Epoch 6 Batch 26 loss is: 0.003056738623046916\n",
      "Epoch 6 Batch 27 loss is: 0.0021201893644306816\n",
      "Epoch 6 Batch 28 loss is: 0.004275288399550234\n",
      "Epoch 6 Batch 29 loss is: 0.003425343681408085\n",
      "Epoch 6 Batch 30 loss is: 0.00321853154297956\n",
      "Epoch 6 Batch 31 loss is: 0.004215700549561229\n",
      "Epoch 6 Batch 32 loss is: 0.0038715234940800277\n",
      "Epoch 6 Batch 33 loss is: 0.004602075501806079\n",
      "Epoch 6 Batch 34 loss is: 0.0026187864732899633\n",
      "Epoch 6 Batch 35 loss is: 0.002571419078109803\n",
      "Epoch 6 Batch 36 loss is: 0.0022647466567987827\n",
      "Epoch 6 Batch 37 loss is: 0.0022091246661381093\n",
      "Epoch 6 Batch 38 loss is: 0.0028013265812915716\n",
      "Epoch 6 Batch 39 loss is: 0.0030479370189102303\n",
      "Epoch 6 Batch 40 loss is: 0.004270296992158269\n",
      "Epoch 6 Batch 41 loss is: 0.003022527573905573\n",
      "Epoch 6 Batch 42 loss is: 0.002808163073992134\n",
      "Epoch 6 Batch 43 loss is: 0.0035585255511629064\n",
      "Epoch 6 Batch 44 loss is: 0.0037878603225592637\n",
      "Epoch 6 Batch 45 loss is: 0.0042147721266441\n",
      "Epoch 6 Batch 46 loss is: 0.0028109379814798057\n",
      "Epoch 6 Batch 47 loss is: 0.0033417511855658214\n",
      "Epoch 6 Batch 48 loss is: 0.004905998052050838\n",
      "Epoch 6 Batch 49 loss is: 0.002379514615289272\n",
      "Epoch 6 Batch 50 loss is: 0.0031377348872506515\n",
      "Epoch 6 Batch 51 loss is: 0.002807999681435026\n",
      "Epoch 6 Batch 52 loss is: 0.002423780679567571\n",
      "Epoch 6 Batch 53 loss is: 0.0021405293994909867\n",
      "Epoch 6 Batch 54 loss is: 0.0025708845060736607\n",
      "Epoch 6 Batch 55 loss is: 0.002388319152488639\n",
      "Epoch 6 Batch 56 loss is: 0.002862167440432254\n",
      "Epoch 6 Batch 57 loss is: 0.004880104183140593\n",
      "Epoch 6 Batch 58 loss is: 0.003996403955962364\n",
      "Epoch 6 Batch 59 loss is: 0.0019570981473453486\n",
      "Epoch 6 Batch 60 loss is: 0.003045740684149673\n",
      "Epoch 6 Batch 61 loss is: 0.004398709631652954\n",
      "Epoch 6 Batch 62 loss is: 0.0025308592219475184\n",
      "Epoch 6 Batch 63 loss is: 0.002892585392533379\n",
      "Epoch 6 Batch 64 loss is: 0.0017713946270666743\n",
      "Epoch 6 Batch 65 loss is: 0.0032855559293672107\n",
      "Epoch 6 Batch 66 loss is: 0.0030644545889558948\n",
      "Epoch 6 Batch 67 loss is: 0.0053598204790589205\n",
      "Epoch 6 Batch 68 loss is: 0.0026352956773925485\n",
      "Epoch 6 Batch 69 loss is: 0.002005503960897504\n",
      "Epoch 6 Batch 70 loss is: 0.0028918549766458784\n",
      "Epoch 6 Batch 71 loss is: 0.0040988166848158515\n",
      "Epoch 6 Batch 72 loss is: 0.0036053923200348947\n",
      "Epoch 6 Batch 73 loss is: 0.0012971382980886403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [32:19<1:46:03, 276.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 6 trn loss is: 0.003437454204452766 val loss is: 0.20497724388560673\n",
      "Epoch 7 Batch 0 loss is: 0.004101312465327851\n",
      "Epoch 7 Batch 1 loss is: 0.002083532807143958\n",
      "Epoch 7 Batch 2 loss is: 0.002497942762134002\n",
      "Epoch 7 Batch 3 loss is: 0.0028983197030788923\n",
      "Epoch 7 Batch 4 loss is: 0.0032545194589927817\n",
      "Epoch 7 Batch 5 loss is: 0.0029340055554646226\n",
      "Epoch 7 Batch 6 loss is: 0.0031379968418912794\n",
      "Epoch 7 Batch 7 loss is: 0.0028349508045461863\n",
      "Epoch 7 Batch 8 loss is: 0.002748673760238501\n",
      "Epoch 7 Batch 9 loss is: 0.0017400216291223577\n",
      "Epoch 7 Batch 10 loss is: 0.002204371476475515\n",
      "Epoch 7 Batch 11 loss is: 0.004716393531226414\n",
      "Epoch 7 Batch 12 loss is: 0.003149824084575575\n",
      "Epoch 7 Batch 13 loss is: 0.001951721540182542\n",
      "Epoch 7 Batch 14 loss is: 0.0021281269218057257\n",
      "Epoch 7 Batch 15 loss is: 0.0022094789171805\n",
      "Epoch 7 Batch 16 loss is: 0.001873957457381484\n",
      "Epoch 7 Batch 17 loss is: 0.0024132751317446593\n",
      "Epoch 7 Batch 18 loss is: 0.004327378481269856\n",
      "Epoch 7 Batch 19 loss is: 0.0012705062521258033\n",
      "Epoch 7 Batch 20 loss is: 0.0025105831034040213\n",
      "Epoch 7 Batch 21 loss is: 0.003285831067744572\n",
      "Epoch 7 Batch 22 loss is: 0.002823896815470803\n",
      "Epoch 7 Batch 23 loss is: 0.0020933690388378304\n",
      "Epoch 7 Batch 24 loss is: 0.003633836645473861\n",
      "Epoch 7 Batch 25 loss is: 0.0018161102643475856\n",
      "Epoch 7 Batch 26 loss is: 0.002088716793073786\n",
      "Epoch 7 Batch 27 loss is: 0.0019347002476780754\n",
      "Epoch 7 Batch 28 loss is: 0.002239195693175399\n",
      "Epoch 7 Batch 29 loss is: 0.002604328853583624\n",
      "Epoch 7 Batch 30 loss is: 0.0026663733831531764\n",
      "Epoch 7 Batch 31 loss is: 0.0035069972070964892\n",
      "Epoch 7 Batch 32 loss is: 0.003380846876406641\n",
      "Epoch 7 Batch 33 loss is: 0.002764111068264512\n",
      "Epoch 7 Batch 34 loss is: 0.0026494496053544956\n",
      "Epoch 7 Batch 35 loss is: 0.0015284455394634715\n",
      "Epoch 7 Batch 36 loss is: 0.002170302757024416\n",
      "Epoch 7 Batch 37 loss is: 0.002362494083644151\n",
      "Epoch 7 Batch 38 loss is: 0.0023112024381839783\n",
      "Epoch 7 Batch 39 loss is: 0.0033222215446790195\n",
      "Epoch 7 Batch 40 loss is: 0.0015424482981471498\n",
      "Epoch 7 Batch 41 loss is: 0.0025897421604936\n",
      "Epoch 7 Batch 42 loss is: 0.0032131091621218034\n",
      "Epoch 7 Batch 43 loss is: 0.002630176289672524\n",
      "Epoch 7 Batch 44 loss is: 0.0027538519169524513\n",
      "Epoch 7 Batch 45 loss is: 0.0028759576844903495\n",
      "Epoch 7 Batch 46 loss is: 0.0027655880000737433\n",
      "Epoch 7 Batch 47 loss is: 0.0029839832787644364\n",
      "Epoch 7 Batch 48 loss is: 0.0015298964965485122\n",
      "Epoch 7 Batch 49 loss is: 0.0021471705261012866\n",
      "Epoch 7 Batch 50 loss is: 0.0017480522510446642\n",
      "Epoch 7 Batch 51 loss is: 0.003518058187864549\n",
      "Epoch 7 Batch 52 loss is: 0.0018057880292894405\n",
      "Epoch 7 Batch 53 loss is: 0.0014133753622473696\n",
      "Epoch 7 Batch 54 loss is: 0.0032234085555815045\n",
      "Epoch 7 Batch 55 loss is: 0.0021711082124762035\n",
      "Epoch 7 Batch 56 loss is: 0.0026899598511314574\n",
      "Epoch 7 Batch 57 loss is: 0.002266598059687605\n",
      "Epoch 7 Batch 58 loss is: 0.0016596776580323318\n",
      "Epoch 7 Batch 59 loss is: 0.0017560872900079616\n",
      "Epoch 7 Batch 60 loss is: 0.0020832812598249955\n",
      "Epoch 7 Batch 61 loss is: 0.0034644635354228845\n",
      "Epoch 7 Batch 62 loss is: 0.0018434993173938826\n",
      "Epoch 7 Batch 63 loss is: 0.0026949304434548084\n",
      "Epoch 7 Batch 64 loss is: 0.002034788380756349\n",
      "Epoch 7 Batch 65 loss is: 0.002282602125696324\n",
      "Epoch 7 Batch 66 loss is: 0.0036334043703908493\n",
      "Epoch 7 Batch 67 loss is: 0.0018270612343337807\n",
      "Epoch 7 Batch 68 loss is: 0.0032469693823300124\n",
      "Epoch 7 Batch 69 loss is: 0.002317341689658387\n",
      "Epoch 7 Batch 70 loss is: 0.0018293484927685692\n",
      "Epoch 7 Batch 71 loss is: 0.0021542743907132927\n",
      "Epoch 7 Batch 72 loss is: 0.002292470478242805\n",
      "Epoch 7 Batch 73 loss is: 0.0025424328873376823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [36:54<1:41:14, 276.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 7 trn loss is: 0.0025364895657705665 val loss is: 0.20622525542908193\n",
      "Epoch 8 Batch 0 loss is: 0.0031434965004646406\n",
      "Epoch 8 Batch 1 loss is: 0.0023727940734443345\n",
      "Epoch 8 Batch 2 loss is: 0.0022794093842719576\n",
      "Epoch 8 Batch 3 loss is: 0.002535660737593825\n",
      "Epoch 8 Batch 4 loss is: 0.002396574123759597\n",
      "Epoch 8 Batch 5 loss is: 0.0026939720765753114\n",
      "Epoch 8 Batch 6 loss is: 0.003742510964371384\n",
      "Epoch 8 Batch 7 loss is: 0.0017789294608166984\n",
      "Epoch 8 Batch 8 loss is: 0.002212332639295932\n",
      "Epoch 8 Batch 9 loss is: 0.0031743205466538645\n",
      "Epoch 8 Batch 10 loss is: 0.0016994850930055823\n",
      "Epoch 8 Batch 11 loss is: 0.0010843667876313152\n",
      "Epoch 8 Batch 12 loss is: 0.002195749976625147\n",
      "Epoch 8 Batch 13 loss is: 0.0013924780892084362\n",
      "Epoch 8 Batch 14 loss is: 0.0018347815465699568\n",
      "Epoch 8 Batch 15 loss is: 0.002095909664469057\n",
      "Epoch 8 Batch 16 loss is: 0.0015153322230972558\n",
      "Epoch 8 Batch 17 loss is: 0.0021125354895381323\n",
      "Epoch 8 Batch 18 loss is: 0.0033233754525434853\n",
      "Epoch 8 Batch 19 loss is: 0.0015142899555526887\n",
      "Epoch 8 Batch 20 loss is: 0.0014851087465804842\n",
      "Epoch 8 Batch 21 loss is: 0.0015606753503360693\n",
      "Epoch 8 Batch 22 loss is: 0.002553691947559547\n",
      "Epoch 8 Batch 23 loss is: 0.0020587016703473775\n",
      "Epoch 8 Batch 24 loss is: 0.0013356907514188046\n",
      "Epoch 8 Batch 25 loss is: 0.0016354454330551249\n",
      "Epoch 8 Batch 26 loss is: 0.0013292115895873972\n",
      "Epoch 8 Batch 27 loss is: 0.002219926452408932\n",
      "Epoch 8 Batch 28 loss is: 0.0012452125967313777\n",
      "Epoch 8 Batch 29 loss is: 0.0015591393876719431\n",
      "Epoch 8 Batch 30 loss is: 0.0012736396716829528\n",
      "Epoch 8 Batch 31 loss is: 0.001909783524457538\n",
      "Epoch 8 Batch 32 loss is: 0.0019152558708921674\n",
      "Epoch 8 Batch 33 loss is: 0.002674850550988168\n",
      "Epoch 8 Batch 34 loss is: 0.00220280078124856\n",
      "Epoch 8 Batch 35 loss is: 0.0021927472388740435\n",
      "Epoch 8 Batch 36 loss is: 0.0015420692284752137\n",
      "Epoch 8 Batch 37 loss is: 0.0014402155454453927\n",
      "Epoch 8 Batch 38 loss is: 0.0023925430432129957\n",
      "Epoch 8 Batch 39 loss is: 0.0014763381957108852\n",
      "Epoch 8 Batch 40 loss is: 0.0008071930512942771\n",
      "Epoch 8 Batch 41 loss is: 0.0014323169630343813\n",
      "Epoch 8 Batch 42 loss is: 0.0024798638063685983\n",
      "Epoch 8 Batch 43 loss is: 0.0011116215133963436\n",
      "Epoch 8 Batch 44 loss is: 0.0020462095034216787\n",
      "Epoch 8 Batch 45 loss is: 0.001429313091155701\n",
      "Epoch 8 Batch 46 loss is: 0.0025229048409435724\n",
      "Epoch 8 Batch 47 loss is: 0.0023768065697235365\n",
      "Epoch 8 Batch 48 loss is: 0.0022095854838101304\n",
      "Epoch 8 Batch 49 loss is: 0.002255947389350747\n",
      "Epoch 8 Batch 50 loss is: 0.0014595802322516249\n",
      "Epoch 8 Batch 51 loss is: 0.00198955739458062\n",
      "Epoch 8 Batch 52 loss is: 0.00232922175489063\n",
      "Epoch 8 Batch 53 loss is: 0.0017883173066702085\n",
      "Epoch 8 Batch 54 loss is: 0.0021969281420666674\n",
      "Epoch 8 Batch 55 loss is: 0.0030712279803147644\n",
      "Epoch 8 Batch 56 loss is: 0.0018681063735246064\n",
      "Epoch 8 Batch 57 loss is: 0.0017731419324782157\n",
      "Epoch 8 Batch 58 loss is: 0.0018782621076286906\n",
      "Epoch 8 Batch 59 loss is: 0.0015857174916586322\n",
      "Epoch 8 Batch 60 loss is: 0.0017839806629632922\n",
      "Epoch 8 Batch 61 loss is: 0.00157309255537605\n",
      "Epoch 8 Batch 62 loss is: 0.001314037101890572\n",
      "Epoch 8 Batch 63 loss is: 0.0016688217624355418\n",
      "Epoch 8 Batch 64 loss is: 0.0010522100665444848\n",
      "Epoch 8 Batch 65 loss is: 0.0020375241003572123\n",
      "Epoch 8 Batch 66 loss is: 0.0021056242505715634\n",
      "Epoch 8 Batch 67 loss is: 0.001938711061019139\n",
      "Epoch 8 Batch 68 loss is: 0.0018263523583476153\n",
      "Epoch 8 Batch 69 loss is: 0.0018105136655846602\n",
      "Epoch 8 Batch 70 loss is: 0.0025047806794449437\n",
      "Epoch 8 Batch 71 loss is: 0.001351012097377075\n",
      "Epoch 8 Batch 72 loss is: 0.001660290214747159\n",
      "Epoch 8 Batch 73 loss is: 0.0015852004996339275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [41:30<1:36:38, 276.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 8 trn loss is: 0.0019449368427977077 val loss is: 0.2123480150169797\n",
      "Epoch 9 Batch 0 loss is: 0.001986331841901586\n",
      "Epoch 9 Batch 1 loss is: 0.0023501830623935877\n",
      "Epoch 9 Batch 2 loss is: 0.0011441118381290967\n",
      "Epoch 9 Batch 3 loss is: 0.0019005513291196982\n",
      "Epoch 9 Batch 4 loss is: 0.0018422606306368057\n",
      "Epoch 9 Batch 5 loss is: 0.0011521928174090827\n",
      "Epoch 9 Batch 6 loss is: 0.003171727422657552\n",
      "Epoch 9 Batch 7 loss is: 0.0018843737556821813\n",
      "Epoch 9 Batch 8 loss is: 0.0013607915254985887\n",
      "Epoch 9 Batch 9 loss is: 0.0018699196242565345\n",
      "Epoch 9 Batch 10 loss is: 0.0015207967031192501\n",
      "Epoch 9 Batch 11 loss is: 0.0010716513636722642\n",
      "Epoch 9 Batch 12 loss is: 0.001966155666805823\n",
      "Epoch 9 Batch 13 loss is: 0.00152709582926074\n",
      "Epoch 9 Batch 14 loss is: 0.0020330973282950047\n",
      "Epoch 9 Batch 15 loss is: 0.0014904641993162926\n",
      "Epoch 9 Batch 16 loss is: 0.0016885151024982766\n",
      "Epoch 9 Batch 17 loss is: 0.0015727090305887497\n",
      "Epoch 9 Batch 18 loss is: 0.001676501108569989\n",
      "Epoch 9 Batch 19 loss is: 0.002056955780882319\n",
      "Epoch 9 Batch 20 loss is: 0.0014422756194328438\n",
      "Epoch 9 Batch 21 loss is: 0.0016142173158808267\n",
      "Epoch 9 Batch 22 loss is: 0.0008871317753712542\n",
      "Epoch 9 Batch 23 loss is: 0.001513913721417713\n",
      "Epoch 9 Batch 24 loss is: 0.0015301170482873753\n",
      "Epoch 9 Batch 25 loss is: 0.0011571845678783176\n",
      "Epoch 9 Batch 26 loss is: 0.001578874218089273\n",
      "Epoch 9 Batch 27 loss is: 0.001952092811626045\n",
      "Epoch 9 Batch 28 loss is: 0.001549975288466759\n",
      "Epoch 9 Batch 29 loss is: 0.0016260591527109132\n",
      "Epoch 9 Batch 30 loss is: 0.0010629181009962233\n",
      "Epoch 9 Batch 31 loss is: 0.0015489291844980595\n",
      "Epoch 9 Batch 32 loss is: 0.001276000166674033\n",
      "Epoch 9 Batch 33 loss is: 0.0014791106570533447\n",
      "Epoch 9 Batch 34 loss is: 0.0009846885051710075\n",
      "Epoch 9 Batch 35 loss is: 0.0014680407353365392\n",
      "Epoch 9 Batch 36 loss is: 0.0023352181793895475\n",
      "Epoch 9 Batch 37 loss is: 0.0017210802554892505\n",
      "Epoch 9 Batch 38 loss is: 0.0016041243929455125\n",
      "Epoch 9 Batch 39 loss is: 0.001445430657823863\n",
      "Epoch 9 Batch 40 loss is: 0.0013404345592469725\n",
      "Epoch 9 Batch 41 loss is: 0.0014154029978977196\n",
      "Epoch 9 Batch 42 loss is: 0.001612670498741172\n",
      "Epoch 9 Batch 43 loss is: 0.002252987775312292\n",
      "Epoch 9 Batch 44 loss is: 0.0014518905372687622\n",
      "Epoch 9 Batch 45 loss is: 0.0016315719983358197\n",
      "Epoch 9 Batch 46 loss is: 0.0016209230875443837\n",
      "Epoch 9 Batch 47 loss is: 0.0012109618266278469\n",
      "Epoch 9 Batch 48 loss is: 0.0017721955306921207\n",
      "Epoch 9 Batch 49 loss is: 0.001885375219768844\n",
      "Epoch 9 Batch 50 loss is: 0.0019733018547449886\n",
      "Epoch 9 Batch 51 loss is: 0.0018739588710161097\n",
      "Epoch 9 Batch 52 loss is: 0.0011177912305623276\n",
      "Epoch 9 Batch 53 loss is: 0.0012409162656880567\n",
      "Epoch 9 Batch 54 loss is: 0.001443273619647485\n",
      "Epoch 9 Batch 55 loss is: 0.0019768148904569334\n",
      "Epoch 9 Batch 56 loss is: 0.001210604611976341\n",
      "Epoch 9 Batch 57 loss is: 0.0009601429152948526\n",
      "Epoch 9 Batch 58 loss is: 0.0017800730109572764\n",
      "Epoch 9 Batch 59 loss is: 0.0017435336188092994\n",
      "Epoch 9 Batch 60 loss is: 0.0011261682966686\n",
      "Epoch 9 Batch 61 loss is: 0.0010023261966519924\n",
      "Epoch 9 Batch 62 loss is: 0.0015188808691461967\n",
      "Epoch 9 Batch 63 loss is: 0.0020199513928823858\n",
      "Epoch 9 Batch 64 loss is: 0.002057058822532814\n",
      "Epoch 9 Batch 65 loss is: 0.0009556484616147998\n",
      "Epoch 9 Batch 66 loss is: 0.0009691213004992012\n",
      "Epoch 9 Batch 67 loss is: 0.0008807143360469164\n",
      "Epoch 9 Batch 68 loss is: 0.0015451474529304457\n",
      "Epoch 9 Batch 69 loss is: 0.0015492079212110828\n",
      "Epoch 9 Batch 70 loss is: 0.001216382713164516\n",
      "Epoch 9 Batch 71 loss is: 0.001226486205807215\n",
      "Epoch 9 Batch 72 loss is: 0.0011237316092043651\n",
      "Epoch 9 Batch 73 loss is: 0.0008892836373526336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [46:04<1:31:47, 275.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 9 trn loss is: 0.0015491716547504717 val loss is: 0.21318182467005364\n",
      "Epoch 10 Batch 0 loss is: 0.0014120930086278113\n",
      "Epoch 10 Batch 1 loss is: 0.0008785628241533535\n",
      "Epoch 10 Batch 2 loss is: 0.0012272200514882078\n",
      "Epoch 10 Batch 3 loss is: 0.0018977565882937596\n",
      "Epoch 10 Batch 4 loss is: 0.0012400651739195468\n",
      "Epoch 10 Batch 5 loss is: 0.0017705731868221145\n",
      "Epoch 10 Batch 6 loss is: 0.0012690060671549474\n",
      "Epoch 10 Batch 7 loss is: 0.0008668498660822622\n",
      "Epoch 10 Batch 8 loss is: 0.0008914964763643951\n",
      "Epoch 10 Batch 9 loss is: 0.0014010753146805636\n",
      "Epoch 10 Batch 10 loss is: 0.0014048225156313284\n",
      "Epoch 10 Batch 11 loss is: 0.000717700283430555\n",
      "Epoch 10 Batch 12 loss is: 0.0013951875664357516\n",
      "Epoch 10 Batch 13 loss is: 0.0017780766079000425\n",
      "Epoch 10 Batch 14 loss is: 0.0014738156166020624\n",
      "Epoch 10 Batch 15 loss is: 0.0010321173591292402\n",
      "Epoch 10 Batch 16 loss is: 0.002049751250878889\n",
      "Epoch 10 Batch 17 loss is: 0.002008101868946284\n",
      "Epoch 10 Batch 18 loss is: 0.0015016730771040443\n",
      "Epoch 10 Batch 19 loss is: 0.0013124345456021777\n",
      "Epoch 10 Batch 20 loss is: 0.001260037336447534\n",
      "Epoch 10 Batch 21 loss is: 0.0015253941127318883\n",
      "Epoch 10 Batch 22 loss is: 0.0009900496745793438\n",
      "Epoch 10 Batch 23 loss is: 0.0006445338028829184\n",
      "Epoch 10 Batch 24 loss is: 0.001880615029524151\n",
      "Epoch 10 Batch 25 loss is: 0.0009894361799576738\n",
      "Epoch 10 Batch 26 loss is: 0.0008060214085383421\n",
      "Epoch 10 Batch 27 loss is: 0.0010798541938908813\n",
      "Epoch 10 Batch 28 loss is: 0.0013748900320582655\n",
      "Epoch 10 Batch 29 loss is: 0.0007026681928003834\n",
      "Epoch 10 Batch 30 loss is: 0.0020339154189356634\n",
      "Epoch 10 Batch 31 loss is: 0.000965062554359785\n",
      "Epoch 10 Batch 32 loss is: 0.0017586176667774823\n",
      "Epoch 10 Batch 33 loss is: 0.002001064865655747\n",
      "Epoch 10 Batch 34 loss is: 0.0013083702016426695\n",
      "Epoch 10 Batch 35 loss is: 0.0012009626183268314\n",
      "Epoch 10 Batch 36 loss is: 0.001370656310986386\n",
      "Epoch 10 Batch 37 loss is: 0.0018933464182511036\n",
      "Epoch 10 Batch 38 loss is: 0.0009814814045643536\n",
      "Epoch 10 Batch 39 loss is: 0.0008649255532596337\n",
      "Epoch 10 Batch 40 loss is: 0.0010644639060703536\n",
      "Epoch 10 Batch 41 loss is: 0.0008320543388831681\n",
      "Epoch 10 Batch 42 loss is: 0.0010947246542129582\n",
      "Epoch 10 Batch 43 loss is: 0.0012858745728014042\n",
      "Epoch 10 Batch 44 loss is: 0.0016006721347048133\n",
      "Epoch 10 Batch 45 loss is: 0.0011106697260084531\n",
      "Epoch 10 Batch 46 loss is: 0.000796331796171709\n",
      "Epoch 10 Batch 47 loss is: 0.0011288057138625854\n",
      "Epoch 10 Batch 48 loss is: 0.0017886485787475218\n",
      "Epoch 10 Batch 49 loss is: 0.0011249481656248387\n",
      "Epoch 10 Batch 50 loss is: 0.0012949346601601756\n",
      "Epoch 10 Batch 51 loss is: 0.0017956185178936816\n",
      "Epoch 10 Batch 52 loss is: 0.0012602090585649725\n",
      "Epoch 10 Batch 53 loss is: 0.001622436769258589\n",
      "Epoch 10 Batch 54 loss is: 0.0016805085845020785\n",
      "Epoch 10 Batch 55 loss is: 0.0012985308918881582\n",
      "Epoch 10 Batch 56 loss is: 0.0012896267262803227\n",
      "Epoch 10 Batch 57 loss is: 0.0009278990711309613\n",
      "Epoch 10 Batch 58 loss is: 0.002037091022159198\n",
      "Epoch 10 Batch 59 loss is: 0.0010105581654607457\n",
      "Epoch 10 Batch 60 loss is: 0.0010531412643070108\n",
      "Epoch 10 Batch 61 loss is: 0.0007820761811858527\n",
      "Epoch 10 Batch 62 loss is: 0.001001884398947747\n",
      "Epoch 10 Batch 63 loss is: 0.0016424789683616582\n",
      "Epoch 10 Batch 64 loss is: 0.0009525092818685958\n",
      "Epoch 10 Batch 65 loss is: 0.0013469106612007428\n",
      "Epoch 10 Batch 66 loss is: 0.001018143299577403\n",
      "Epoch 10 Batch 67 loss is: 0.0016054828112560908\n",
      "Epoch 10 Batch 68 loss is: 0.0020026661489200375\n",
      "Epoch 10 Batch 69 loss is: 0.0012815182947858972\n",
      "Epoch 10 Batch 70 loss is: 0.000781089955798322\n",
      "Epoch 10 Batch 71 loss is: 0.0016486380402086808\n",
      "Epoch 10 Batch 72 loss is: 0.0010720373618210743\n",
      "Epoch 10 Batch 73 loss is: 0.0010444880199510466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [50:39<1:27:12, 275.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 10 trn loss is: 0.001303215594162098 val loss is: 0.21152194913135589\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f64e3509672c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mrunning_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m#Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "fold = 0\n",
    "\n",
    "for train_index, val_index in skf.split(np.zeros(len(train.int_labels)), train.int_labels):\n",
    "    train_subset = torch.utils.data.Subset(train, train_index)\n",
    "    val_subset = torch.utils.data.Subset(train, val_index)\n",
    "    \n",
    "    #initialize data loaders using subsets\n",
    "    dataloader_train = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    dataloader_val = DataLoader(val_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    #Reinitialize network for every instance\n",
    "    net = torch_classes.Net(num_of_classes=33)\n",
    "    net.to(device)\n",
    "    net.double()\n",
    "    \n",
    "    #Training loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #Training optimizer\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    fold_trn_loss = []\n",
    "    fold_val_loss = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        for i, samples in enumerate(dataloader_train):\n",
    "            #Data\n",
    "            images = samples['data'].to(device).requires_grad_()\n",
    "            images = torch.unsqueeze(images,1)\n",
    "            labels = samples['label'].to(device)\n",
    "            _, labels = torch.max(labels,1) #converts from one hot to integer\n",
    "            \n",
    "            #Forward Pass\n",
    "            outputs = net(images)\n",
    "\n",
    "            #Backward Pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.data\n",
    "            print(\"Epoch {} Batch {} loss is: {}\".format(epoch, i, loss.data))\n",
    "            \n",
    "        epoch_training_loss = running_loss.item()/(i+1) #i + 1 since index starts from 0\n",
    "        \n",
    "        for j, val_samples in enumerate(dataloader_val):\n",
    "            #validation loss\n",
    "            images = val_samples['data'].to(device)\n",
    "            images = torch.unsqueeze(images,1)\n",
    "            labels = val_samples['label'].to(device)\n",
    "            _, labels = torch.max(labels,1)\n",
    "\n",
    "            #Forward Pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.data\n",
    "            \n",
    "        epoch_validation_loss = running_val_loss.item()/(j+1)\n",
    "        \n",
    "        fold_trn_loss.append(epoch_training_loss)\n",
    "        fold_val_loss.append(epoch_validation_loss)\n",
    "        print('fold', fold, 'epoch', epoch, 'trn loss is:', epoch_training_loss, 'val loss is:', epoch_validation_loss)\n",
    "    \n",
    "    plt.plot(fold_trn_loss, label=\"Train\")\n",
    "    plt.plot(fold_val_loss, label=\"Validation\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (Cross Entropy)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    training_losses.append(fold_trn_loss)\n",
    "    validation_losses.append(fold_val_loss)\n",
    "    fold = fold + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is list of lists of lists\n",
    "losses = [training_losses,validation_losses]\n",
    "with open('CV_file/losses/losses.pkl', 'wb') as f:\n",
    "    pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(len(training_losses)):\n",
    "    plt.subplot(len(training_losses)+1,1,i+1)\n",
    "    plt.plot(training_losses[i], label=\"Train\")\n",
    "    plt.plot(validation_losses[i], label=\"Validation\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (Cross Entropy)')\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "fig.set_size_inches(8, 20)    \n",
    "fig.savefig(\"CV_file/Loss.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
