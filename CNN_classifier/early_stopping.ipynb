{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from modules import torch_classes\n",
    "from torchvision import transforms, utils\n",
    "from modules.grad_cam import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set\n",
    "labels_path = '../DATA/train_labels.pkl'\n",
    "data_path = 'train.dat'\n",
    "data_dims = (8269,10404)\n",
    "genes_path = 'train.csv'\n",
    "\n",
    "#Data and labels\n",
    "train = torch_classes.TumorDataset(labels_path,data_path,data_dims,genes_path,transform = transforms.Compose([torch_classes.ToImage(),torch_classes.ToTensor()]))\n",
    "\n",
    "#Test set\n",
    "labels_path = '../DATA/test_labels.pkl'\n",
    "data_path = 'test.dat'\n",
    "data_dims = (2085,10404)\n",
    "genes_path = 'test.csv'\n",
    "\n",
    "test = torch_classes.TumorDataset(labels_path,data_path,data_dims,genes_path,transform = transforms.Compose([torch_classes.ToImage(),torch_classes.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 75\n",
    "learning_rate = 0.0001\n",
    "\n",
    "net = torch_classes.Net(num_of_classes=33)\n",
    "net.to(device)\n",
    "net.double()\n",
    "\n",
    "#Training loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Training optimizer\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15)\n",
    "\n",
    "#Single loop to get indicies\n",
    "for train_index, val_index in sss.split(np.zeros(len(train.int_labels)), train.int_labels):\n",
    "    train_subset = torch.utils.data.Subset(train, train_index)\n",
    "    val_subset = torch.utils.data.Subset(train, val_index)\n",
    "    \n",
    "#initialize data loaders using subsets\n",
    "dataloader_train = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader_val = DataLoader(val_subset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0 loss is: 3.518521642254989\n",
      "Epoch 0 Batch 1 loss is: 3.8712316085365654\n",
      "Epoch 0 Batch 2 loss is: 3.758162479418121\n",
      "Epoch 0 Batch 3 loss is: 3.010863333427325\n",
      "Epoch 0 Batch 4 loss is: 2.8834931909197303\n",
      "Epoch 0 Batch 5 loss is: 2.7282659703670475\n",
      "Epoch 0 Batch 6 loss is: 2.5472590976950875\n",
      "Epoch 0 Batch 7 loss is: 2.421582608740413\n",
      "Epoch 0 Batch 8 loss is: 2.340670277796199\n",
      "Epoch 0 Batch 9 loss is: 1.9347495236049523\n",
      "Epoch 0 Batch 10 loss is: 1.9101258586908905\n",
      "Epoch 0 Batch 11 loss is: 1.989105378318807\n",
      "Epoch 0 Batch 12 loss is: 1.9776118401771139\n",
      "Epoch 0 Batch 13 loss is: 1.777035872639607\n",
      "Epoch 0 Batch 14 loss is: 1.6195798734507103\n",
      "Epoch 0 Batch 15 loss is: 1.5927843424120072\n",
      "Epoch 0 Batch 16 loss is: 1.5080177131561887\n",
      "Epoch 0 Batch 17 loss is: 1.6767222727746214\n",
      "Epoch 0 Batch 18 loss is: 1.5160949423773311\n",
      "Epoch 0 Batch 19 loss is: 1.5239111896529707\n",
      "Epoch 0 Batch 20 loss is: 1.5049549563826186\n",
      "Epoch 0 Batch 21 loss is: 1.1292033984866316\n",
      "Epoch 0 Batch 22 loss is: 1.3935490872909977\n",
      "Epoch 0 Batch 23 loss is: 1.4452813903057071\n",
      "Epoch 0 Batch 24 loss is: 1.371750018599724\n",
      "Epoch 0 Batch 25 loss is: 1.0520900982167989\n",
      "Epoch 0 Batch 26 loss is: 0.9485152631136797\n",
      "Epoch 0 Batch 27 loss is: 1.292543223993399\n",
      "Epoch 0 Batch 28 loss is: 1.1515116747950758\n",
      "Epoch 0 Batch 29 loss is: 1.0655118628843425\n",
      "Epoch 0 Batch 30 loss is: 0.985948725716396\n",
      "Epoch 0 Batch 31 loss is: 0.9229846193539534\n",
      "Epoch 0 Batch 32 loss is: 0.715616558115826\n",
      "Epoch 0 Batch 33 loss is: 0.8372533199985254\n",
      "Epoch 0 Batch 34 loss is: 1.1523648666999229\n",
      "Epoch 0 Batch 35 loss is: 0.7828648914403542\n",
      "Epoch 0 Batch 36 loss is: 0.9163717074788054\n",
      "Epoch 0 Batch 37 loss is: 0.9776067205637581\n",
      "Epoch 0 Batch 38 loss is: 0.9450017765021979\n",
      "Epoch 0 Batch 39 loss is: 0.9323976017134441\n",
      "Epoch 0 Batch 40 loss is: 0.7649836783657317\n",
      "Epoch 0 Batch 41 loss is: 0.48592523372650787\n",
      "Epoch 0 Batch 42 loss is: 0.747373622797427\n",
      "Epoch 0 Batch 43 loss is: 0.6437749186700668\n",
      "Epoch 0 Batch 44 loss is: 0.6511613143244648\n",
      "Epoch 0 Batch 45 loss is: 0.47010809373762497\n",
      "Epoch 0 Batch 46 loss is: 0.6255903955920153\n",
      "Epoch 0 Batch 47 loss is: 0.43870450971970815\n",
      "Epoch 0 Batch 48 loss is: 0.6870142295238237\n",
      "Epoch 0 Batch 49 loss is: 0.4516431523401574\n",
      "Epoch 0 Batch 50 loss is: 0.45853084834481717\n",
      "Epoch 0 Batch 51 loss is: 0.45931784727043606\n",
      "Epoch 0 Batch 52 loss is: 0.40386668397852726\n",
      "Epoch 0 Batch 53 loss is: 0.5907769360151117\n",
      "Epoch 0 Batch 54 loss is: 0.4370360966021097\n",
      "Epoch 0 Batch 55 loss is: 0.5636558842699524\n",
      "Epoch 0 Batch 56 loss is: 0.4215790292071198\n",
      "Epoch 0 Batch 57 loss is: 0.5274246118154836\n",
      "Epoch 0 Batch 58 loss is: 0.3958665230971304\n",
      "Epoch 0 Batch 59 loss is: 0.6506062630678476\n",
      "Epoch 0 Batch 60 loss is: 0.5063051455842659\n",
      "Epoch 0 Batch 61 loss is: 0.6328956372038516\n",
      "Epoch 0 Batch 62 loss is: 0.46668686270657794\n",
      "Epoch 0 Batch 63 loss is: 0.421362230945843\n",
      "Epoch 0 Batch 64 loss is: 0.41659044430328346\n",
      "Epoch 0 Batch 65 loss is: 0.3702989225960999\n",
      "Epoch 0 Batch 66 loss is: 0.6775177805677042\n",
      "Epoch 0 Batch 67 loss is: 0.4735729248340319\n",
      "Epoch 0 Batch 68 loss is: 0.8772439764753871\n",
      "Epoch 0 Batch 69 loss is: 0.28577169551394843\n",
      "Epoch 0 Batch 70 loss is: 0.41534228421607255\n",
      "Epoch 0 Batch 71 loss is: 0.42996192994059595\n",
      "Epoch 0 Batch 72 loss is: 0.3261438113681086\n",
      "Epoch 0 Batch 73 loss is: 0.45288436679243727\n",
      "Epoch 0 Batch 74 loss is: 0.37877269642370814\n",
      "Epoch 0 Batch 75 loss is: 0.41270198240576833\n",
      "Epoch 0 Batch 76 loss is: 0.5063446255244645\n",
      "Epoch 0 Batch 77 loss is: 0.2660132934400241\n",
      "Epoch 0 Batch 78 loss is: 0.3881962929248088\n",
      "Epoch 0 Batch 79 loss is: 0.2938511388775935\n",
      "Epoch 0 Batch 80 loss is: 0.3192306139124834\n",
      "Epoch 0 Batch 81 loss is: 0.5722196197053484\n",
      "Epoch 0 Batch 82 loss is: 0.4218567734317626\n",
      "Epoch 0 Batch 83 loss is: 0.346602066303775\n",
      "Epoch 0 Batch 84 loss is: 0.355056621488499\n",
      "Epoch 0 Batch 85 loss is: 0.20541762049641946\n",
      "Epoch 0 Batch 86 loss is: 0.42310469449430804\n",
      "Epoch 0 Batch 87 loss is: 0.21927837810058662\n",
      "Epoch 0 Batch 88 loss is: 0.3560204778395367\n",
      "Epoch 0 Batch 89 loss is: 0.4590261015741967\n",
      "Epoch 0 Batch 90 loss is: 0.2731249576518124\n",
      "Epoch 0 Batch 91 loss is: 0.4914702418042788\n",
      "Epoch 0 Batch 92 loss is: 0.4616340335398504\n",
      "Epoch 0 Batch 93 loss is: 0.2664397443773551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 1/30 [05:25<2:37:24, 325.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 trn loss is: 0.9858828791265282 val loss is: 0.324025127160218\n",
      "Epoch 1 Batch 0 loss is: 0.3168642864457478\n",
      "Epoch 1 Batch 1 loss is: 0.15662032296944073\n",
      "Epoch 1 Batch 2 loss is: 0.17473186596717677\n",
      "Epoch 1 Batch 3 loss is: 0.18570017552929396\n",
      "Epoch 1 Batch 4 loss is: 0.25425772489217413\n",
      "Epoch 1 Batch 5 loss is: 0.1728998121589691\n",
      "Epoch 1 Batch 6 loss is: 0.16132577270652376\n",
      "Epoch 1 Batch 7 loss is: 0.23149082238690985\n",
      "Epoch 1 Batch 8 loss is: 0.1973013082365515\n",
      "Epoch 1 Batch 9 loss is: 0.2071432819473308\n",
      "Epoch 1 Batch 10 loss is: 0.1827564199520861\n",
      "Epoch 1 Batch 11 loss is: 0.15438469742783578\n",
      "Epoch 1 Batch 12 loss is: 0.16595231833719337\n",
      "Epoch 1 Batch 13 loss is: 0.2757961729871755\n",
      "Epoch 1 Batch 14 loss is: 0.18486318400048707\n",
      "Epoch 1 Batch 15 loss is: 0.16223833195986612\n",
      "Epoch 1 Batch 16 loss is: 0.23300447334119043\n",
      "Epoch 1 Batch 17 loss is: 0.11112117974885233\n",
      "Epoch 1 Batch 18 loss is: 0.16215076065295192\n",
      "Epoch 1 Batch 19 loss is: 0.2143294810227289\n",
      "Epoch 1 Batch 20 loss is: 0.10774892731978336\n",
      "Epoch 1 Batch 21 loss is: 0.13196363641018208\n",
      "Epoch 1 Batch 22 loss is: 0.12139046871302069\n",
      "Epoch 1 Batch 23 loss is: 0.22597239558833407\n",
      "Epoch 1 Batch 24 loss is: 0.1731619541953717\n",
      "Epoch 1 Batch 25 loss is: 0.15804124545362638\n",
      "Epoch 1 Batch 26 loss is: 0.1859994106674859\n",
      "Epoch 1 Batch 27 loss is: 0.15977264220489276\n",
      "Epoch 1 Batch 28 loss is: 0.14830692522421254\n",
      "Epoch 1 Batch 29 loss is: 0.20414834628096096\n",
      "Epoch 1 Batch 30 loss is: 0.15083180091565707\n",
      "Epoch 1 Batch 31 loss is: 0.27137951128969046\n",
      "Epoch 1 Batch 32 loss is: 0.08877080598021159\n",
      "Epoch 1 Batch 33 loss is: 0.23374935970343358\n",
      "Epoch 1 Batch 34 loss is: 0.1566522890763002\n",
      "Epoch 1 Batch 35 loss is: 0.2269759202017962\n",
      "Epoch 1 Batch 36 loss is: 0.178124095404367\n",
      "Epoch 1 Batch 37 loss is: 0.1515365943213918\n",
      "Epoch 1 Batch 38 loss is: 0.1346500863858103\n",
      "Epoch 1 Batch 39 loss is: 0.06068151255431947\n",
      "Epoch 1 Batch 40 loss is: 0.13894240489998305\n",
      "Epoch 1 Batch 41 loss is: 0.10139216175246032\n",
      "Epoch 1 Batch 42 loss is: 0.07299979375830432\n",
      "Epoch 1 Batch 43 loss is: 0.08407696494218068\n",
      "Epoch 1 Batch 44 loss is: 0.2066103258810872\n",
      "Epoch 1 Batch 45 loss is: 0.10918740966694372\n",
      "Epoch 1 Batch 46 loss is: 0.19744374693591182\n",
      "Epoch 1 Batch 47 loss is: 0.2273230445712735\n",
      "Epoch 1 Batch 48 loss is: 0.17314909908016404\n",
      "Epoch 1 Batch 49 loss is: 0.15524301287161107\n",
      "Epoch 1 Batch 50 loss is: 0.19482420835127748\n",
      "Epoch 1 Batch 51 loss is: 0.06888926759455845\n",
      "Epoch 1 Batch 52 loss is: 0.1290463455945176\n",
      "Epoch 1 Batch 53 loss is: 0.19644916509377566\n",
      "Epoch 1 Batch 54 loss is: 0.08828203572631914\n",
      "Epoch 1 Batch 55 loss is: 0.11326672860173635\n",
      "Epoch 1 Batch 56 loss is: 0.07835433967297646\n",
      "Epoch 1 Batch 57 loss is: 0.21503368336886353\n",
      "Epoch 1 Batch 58 loss is: 0.18114268002548173\n",
      "Epoch 1 Batch 59 loss is: 0.10986556440679758\n",
      "Epoch 1 Batch 60 loss is: 0.17128351748795131\n",
      "Epoch 1 Batch 61 loss is: 0.18985179813628278\n",
      "Epoch 1 Batch 62 loss is: 0.10476781756962453\n",
      "Epoch 1 Batch 63 loss is: 0.09341104055252968\n",
      "Epoch 1 Batch 64 loss is: 0.1336699907217525\n",
      "Epoch 1 Batch 65 loss is: 0.15559047103326742\n",
      "Epoch 1 Batch 66 loss is: 0.20775870364647825\n",
      "Epoch 1 Batch 67 loss is: 0.21436068074610518\n",
      "Epoch 1 Batch 68 loss is: 0.25425242867867937\n",
      "Epoch 1 Batch 69 loss is: 0.1357531291714865\n",
      "Epoch 1 Batch 70 loss is: 0.10986160570027628\n",
      "Epoch 1 Batch 71 loss is: 0.14522505267632085\n",
      "Epoch 1 Batch 72 loss is: 0.19389084393627545\n",
      "Epoch 1 Batch 73 loss is: 0.0679029662321209\n",
      "Epoch 1 Batch 74 loss is: 0.15354428303382367\n",
      "Epoch 1 Batch 75 loss is: 0.14683586206553756\n",
      "Epoch 1 Batch 76 loss is: 0.25284292619540905\n",
      "Epoch 1 Batch 77 loss is: 0.12928205686223918\n",
      "Epoch 1 Batch 78 loss is: 0.09369598960275032\n",
      "Epoch 1 Batch 79 loss is: 0.17550307301164883\n",
      "Epoch 1 Batch 80 loss is: 0.1066388468615247\n",
      "Epoch 1 Batch 81 loss is: 0.14278040296001068\n",
      "Epoch 1 Batch 82 loss is: 0.05956208836522021\n",
      "Epoch 1 Batch 83 loss is: 0.09563410497863212\n",
      "Epoch 1 Batch 84 loss is: 0.1389691156503299\n",
      "Epoch 1 Batch 85 loss is: 0.1340342702885616\n",
      "Epoch 1 Batch 86 loss is: 0.2699876191988961\n",
      "Epoch 1 Batch 87 loss is: 0.16268359248978542\n",
      "Epoch 1 Batch 88 loss is: 0.24370986399598024\n",
      "Epoch 1 Batch 89 loss is: 0.1587073559606108\n",
      "Epoch 1 Batch 90 loss is: 0.053232180811857016\n",
      "Epoch 1 Batch 91 loss is: 0.20482790277031296\n",
      "Epoch 1 Batch 92 loss is: 0.19387565841845064\n",
      "Epoch 1 Batch 93 loss is: 0.07867107393778995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 2/30 [10:47<2:31:25, 324.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 trn loss is: 0.16158415550112853 val loss is: 0.23817242570842953\n",
      "Epoch 2 Batch 0 loss is: 0.0506625638822142\n",
      "Epoch 2 Batch 1 loss is: 0.04729554993876312\n",
      "Epoch 2 Batch 2 loss is: 0.07088883265576168\n",
      "Epoch 2 Batch 3 loss is: 0.048792079525662814\n",
      "Epoch 2 Batch 4 loss is: 0.07305927993488105\n",
      "Epoch 2 Batch 5 loss is: 0.03343409158850915\n",
      "Epoch 2 Batch 6 loss is: 0.061317470342308104\n",
      "Epoch 2 Batch 7 loss is: 0.049329274595795995\n",
      "Epoch 2 Batch 8 loss is: 0.07559569332557486\n",
      "Epoch 2 Batch 9 loss is: 0.03330100525382346\n",
      "Epoch 2 Batch 10 loss is: 0.08857022865556168\n",
      "Epoch 2 Batch 11 loss is: 0.05417088136668831\n",
      "Epoch 2 Batch 12 loss is: 0.08043171256482279\n",
      "Epoch 2 Batch 13 loss is: 0.039293178339807194\n",
      "Epoch 2 Batch 14 loss is: 0.045295200532208015\n",
      "Epoch 2 Batch 15 loss is: 0.05929021817565861\n",
      "Epoch 2 Batch 16 loss is: 0.06986502359677241\n",
      "Epoch 2 Batch 17 loss is: 0.10945911839879763\n",
      "Epoch 2 Batch 18 loss is: 0.05977618208380431\n",
      "Epoch 2 Batch 19 loss is: 0.07749219448001626\n",
      "Epoch 2 Batch 20 loss is: 0.03129912123870258\n",
      "Epoch 2 Batch 21 loss is: 0.06684030546710895\n",
      "Epoch 2 Batch 22 loss is: 0.13305412460760072\n",
      "Epoch 2 Batch 23 loss is: 0.05224662495849645\n",
      "Epoch 2 Batch 24 loss is: 0.08280212036577143\n",
      "Epoch 2 Batch 25 loss is: 0.05973264338170783\n",
      "Epoch 2 Batch 26 loss is: 0.027388527170703027\n",
      "Epoch 2 Batch 27 loss is: 0.07517290563978465\n",
      "Epoch 2 Batch 28 loss is: 0.0681176025160673\n",
      "Epoch 2 Batch 29 loss is: 0.04941941375487907\n",
      "Epoch 2 Batch 30 loss is: 0.024045071828782624\n",
      "Epoch 2 Batch 31 loss is: 0.08723380384748597\n",
      "Epoch 2 Batch 32 loss is: 0.05038036630685539\n",
      "Epoch 2 Batch 33 loss is: 0.028078560819064177\n",
      "Epoch 2 Batch 34 loss is: 0.04208391101053653\n",
      "Epoch 2 Batch 35 loss is: 0.027449906426448047\n",
      "Epoch 2 Batch 36 loss is: 0.0365280090021605\n",
      "Epoch 2 Batch 37 loss is: 0.027200687776011836\n",
      "Epoch 2 Batch 38 loss is: 0.07815844128212257\n",
      "Epoch 2 Batch 39 loss is: 0.030917278289421982\n",
      "Epoch 2 Batch 40 loss is: 0.04905389356874122\n",
      "Epoch 2 Batch 41 loss is: 0.03575879765957904\n",
      "Epoch 2 Batch 42 loss is: 0.027436550363162483\n",
      "Epoch 2 Batch 43 loss is: 0.059687541809500456\n",
      "Epoch 2 Batch 44 loss is: 0.04859543467153104\n",
      "Epoch 2 Batch 45 loss is: 0.03700451313931243\n",
      "Epoch 2 Batch 46 loss is: 0.061370906325727115\n",
      "Epoch 2 Batch 47 loss is: 0.03915657839073939\n",
      "Epoch 2 Batch 48 loss is: 0.01981922894516424\n",
      "Epoch 2 Batch 49 loss is: 0.04421733460664662\n",
      "Epoch 2 Batch 50 loss is: 0.04534199154984352\n",
      "Epoch 2 Batch 51 loss is: 0.030256777428873546\n",
      "Epoch 2 Batch 52 loss is: 0.05027025014751789\n",
      "Epoch 2 Batch 53 loss is: 0.030907947576586256\n",
      "Epoch 2 Batch 54 loss is: 0.06113286973611254\n",
      "Epoch 2 Batch 55 loss is: 0.025618150554111086\n",
      "Epoch 2 Batch 56 loss is: 0.02122131967446801\n",
      "Epoch 2 Batch 57 loss is: 0.028191996879833214\n",
      "Epoch 2 Batch 58 loss is: 0.0540590326159541\n",
      "Epoch 2 Batch 59 loss is: 0.08364321410176065\n",
      "Epoch 2 Batch 60 loss is: 0.04539994173065726\n",
      "Epoch 2 Batch 61 loss is: 0.07241123304086736\n",
      "Epoch 2 Batch 62 loss is: 0.06787654368531784\n",
      "Epoch 2 Batch 63 loss is: 0.0564022060176745\n",
      "Epoch 2 Batch 64 loss is: 0.03443888010771903\n",
      "Epoch 2 Batch 65 loss is: 0.026805441307044594\n",
      "Epoch 2 Batch 66 loss is: 0.07208478261412318\n",
      "Epoch 2 Batch 67 loss is: 0.030733649333882895\n",
      "Epoch 2 Batch 68 loss is: 0.04643469712606271\n",
      "Epoch 2 Batch 69 loss is: 0.03541842045710524\n",
      "Epoch 2 Batch 70 loss is: 0.08708442478550356\n",
      "Epoch 2 Batch 71 loss is: 0.04334157696838264\n",
      "Epoch 2 Batch 72 loss is: 0.038557798695731324\n",
      "Epoch 2 Batch 73 loss is: 0.041406374119530985\n",
      "Epoch 2 Batch 74 loss is: 0.0823458949305509\n",
      "Epoch 2 Batch 75 loss is: 0.05665803615584091\n",
      "Epoch 2 Batch 76 loss is: 0.014309659147489337\n",
      "Epoch 2 Batch 77 loss is: 0.036586728275680706\n",
      "Epoch 2 Batch 78 loss is: 0.06962729167240335\n",
      "Epoch 2 Batch 79 loss is: 0.09282490173914991\n",
      "Epoch 2 Batch 80 loss is: 0.04673739109952923\n",
      "Epoch 2 Batch 81 loss is: 0.06782118235966411\n",
      "Epoch 2 Batch 82 loss is: 0.038848615819322786\n",
      "Epoch 2 Batch 83 loss is: 0.023518075893094507\n",
      "Epoch 2 Batch 84 loss is: 0.06273852083051465\n",
      "Epoch 2 Batch 85 loss is: 0.04497255724456731\n",
      "Epoch 2 Batch 86 loss is: 0.025011352102033525\n",
      "Epoch 2 Batch 87 loss is: 0.044963121894713694\n",
      "Epoch 2 Batch 88 loss is: 0.08321038168050604\n",
      "Epoch 2 Batch 89 loss is: 0.016697865286677428\n",
      "Epoch 2 Batch 90 loss is: 0.04453202745333621\n",
      "Epoch 2 Batch 91 loss is: 0.05061930876997613\n",
      "Epoch 2 Batch 92 loss is: 0.03691844046322402\n",
      "Epoch 2 Batch 93 loss is: 0.03291036381951216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 3/30 [16:08<2:25:30, 323.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 trn loss is: 0.051366608737230826 val loss is: 0.1892537194475813\n",
      "Epoch 3 Batch 0 loss is: 0.013629628188310896\n",
      "Epoch 3 Batch 1 loss is: 0.010914982311761818\n",
      "Epoch 3 Batch 2 loss is: 0.01097779108637346\n",
      "Epoch 3 Batch 3 loss is: 0.035916224192536664\n",
      "Epoch 3 Batch 4 loss is: 0.009554402631258832\n",
      "Epoch 3 Batch 5 loss is: 0.06234674656347818\n",
      "Epoch 3 Batch 6 loss is: 0.03711202746383142\n",
      "Epoch 3 Batch 7 loss is: 0.02314872509862949\n",
      "Epoch 3 Batch 8 loss is: 0.012632863775477444\n",
      "Epoch 3 Batch 9 loss is: 0.016246075048026422\n",
      "Epoch 3 Batch 10 loss is: 0.03167317015383263\n",
      "Epoch 3 Batch 11 loss is: 0.031158778990014884\n",
      "Epoch 3 Batch 12 loss is: 0.020456973466155427\n",
      "Epoch 3 Batch 13 loss is: 0.01625877559764452\n",
      "Epoch 3 Batch 14 loss is: 0.0189253825972639\n",
      "Epoch 3 Batch 15 loss is: 0.014378094066169212\n",
      "Epoch 3 Batch 16 loss is: 0.008739748792454047\n",
      "Epoch 3 Batch 17 loss is: 0.017721036603493504\n",
      "Epoch 3 Batch 18 loss is: 0.02649847988238405\n",
      "Epoch 3 Batch 19 loss is: 0.009309155523682193\n",
      "Epoch 3 Batch 20 loss is: 0.011827341080717962\n",
      "Epoch 3 Batch 21 loss is: 0.014783681692162461\n",
      "Epoch 3 Batch 22 loss is: 0.024441787007863796\n",
      "Epoch 3 Batch 23 loss is: 0.03502497991014579\n",
      "Epoch 3 Batch 24 loss is: 0.012107943058872254\n",
      "Epoch 3 Batch 25 loss is: 0.010117504559681325\n",
      "Epoch 3 Batch 26 loss is: 0.011099286272559895\n",
      "Epoch 3 Batch 27 loss is: 0.01074940989683185\n",
      "Epoch 3 Batch 28 loss is: 0.008528412591036948\n",
      "Epoch 3 Batch 29 loss is: 0.016900566256424637\n",
      "Epoch 3 Batch 30 loss is: 0.012958633416052703\n",
      "Epoch 3 Batch 31 loss is: 0.01195272546648327\n",
      "Epoch 3 Batch 32 loss is: 0.02252025568631236\n",
      "Epoch 3 Batch 33 loss is: 0.027263117483719426\n",
      "Epoch 3 Batch 34 loss is: 0.007432802856225322\n",
      "Epoch 3 Batch 35 loss is: 0.0087922835491241\n",
      "Epoch 3 Batch 36 loss is: 0.013413931917664594\n",
      "Epoch 3 Batch 37 loss is: 0.008012992981391171\n",
      "Epoch 3 Batch 38 loss is: 0.01967987757557322\n",
      "Epoch 3 Batch 39 loss is: 0.01607418045647793\n",
      "Epoch 3 Batch 40 loss is: 0.01642315513984027\n",
      "Epoch 3 Batch 41 loss is: 0.013141270723716344\n",
      "Epoch 3 Batch 42 loss is: 0.01956497539571328\n",
      "Epoch 3 Batch 43 loss is: 0.0070501845212652845\n",
      "Epoch 3 Batch 44 loss is: 0.008884430465147943\n",
      "Epoch 3 Batch 45 loss is: 0.009557441218778162\n",
      "Epoch 3 Batch 46 loss is: 0.011262036959628527\n",
      "Epoch 3 Batch 47 loss is: 0.020519448275664553\n",
      "Epoch 3 Batch 48 loss is: 0.010659731965373875\n",
      "Epoch 3 Batch 49 loss is: 0.018288953678679516\n",
      "Epoch 3 Batch 50 loss is: 0.010324721965631257\n",
      "Epoch 3 Batch 51 loss is: 0.017004080721629126\n",
      "Epoch 3 Batch 52 loss is: 0.010707617629091004\n",
      "Epoch 3 Batch 53 loss is: 0.010335225622255612\n",
      "Epoch 3 Batch 54 loss is: 0.007627122701387089\n",
      "Epoch 3 Batch 55 loss is: 0.015574837975398891\n",
      "Epoch 3 Batch 56 loss is: 0.011502871807535632\n",
      "Epoch 3 Batch 57 loss is: 0.009315189097872338\n",
      "Epoch 3 Batch 58 loss is: 0.053394176969949476\n",
      "Epoch 3 Batch 59 loss is: 0.01773861047170583\n",
      "Epoch 3 Batch 60 loss is: 0.011730511001827027\n",
      "Epoch 3 Batch 61 loss is: 0.007496982296849571\n",
      "Epoch 3 Batch 62 loss is: 0.007328143118128724\n",
      "Epoch 3 Batch 63 loss is: 0.04289522454852136\n",
      "Epoch 3 Batch 64 loss is: 0.013322168486418135\n",
      "Epoch 3 Batch 65 loss is: 0.013495796433892279\n",
      "Epoch 3 Batch 66 loss is: 0.02032725764334515\n",
      "Epoch 3 Batch 67 loss is: 0.0096256959857694\n",
      "Epoch 3 Batch 68 loss is: 0.015607163628054282\n",
      "Epoch 3 Batch 69 loss is: 0.009370956595043082\n",
      "Epoch 3 Batch 70 loss is: 0.014038994380565838\n",
      "Epoch 3 Batch 71 loss is: 0.01433430811928428\n",
      "Epoch 3 Batch 72 loss is: 0.011685816586428857\n",
      "Epoch 3 Batch 73 loss is: 0.008869920887248112\n",
      "Epoch 3 Batch 74 loss is: 0.009314688121912091\n",
      "Epoch 3 Batch 75 loss is: 0.008784718644575746\n",
      "Epoch 3 Batch 76 loss is: 0.015786719586152718\n",
      "Epoch 3 Batch 77 loss is: 0.011796711481584374\n",
      "Epoch 3 Batch 78 loss is: 0.008238551491383745\n",
      "Epoch 3 Batch 79 loss is: 0.012455297712374929\n",
      "Epoch 3 Batch 80 loss is: 0.009732532875829432\n",
      "Epoch 3 Batch 81 loss is: 0.013057795654767203\n",
      "Epoch 3 Batch 82 loss is: 0.09442926369136348\n",
      "Epoch 3 Batch 83 loss is: 0.01709565193299182\n",
      "Epoch 3 Batch 84 loss is: 0.006922194538294365\n",
      "Epoch 3 Batch 85 loss is: 0.02445491906025834\n",
      "Epoch 3 Batch 86 loss is: 0.016439658051551228\n",
      "Epoch 3 Batch 87 loss is: 0.019252262748049156\n",
      "Epoch 3 Batch 88 loss is: 0.01238360574610572\n",
      "Epoch 3 Batch 89 loss is: 0.01684879417021939\n",
      "Epoch 3 Batch 90 loss is: 0.016656024828659036\n",
      "Epoch 3 Batch 91 loss is: 0.013440768798153353\n",
      "Epoch 3 Batch 92 loss is: 0.015271057822856164\n",
      "Epoch 3 Batch 93 loss is: 0.018142240666003874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 4/30 [21:28<2:19:41, 322.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 trn loss is: 0.016923289961583265 val loss is: 0.17467267249095905\n",
      "Epoch 4 Batch 0 loss is: 0.007270413293278999\n",
      "Epoch 4 Batch 1 loss is: 0.003898265546206545\n",
      "Epoch 4 Batch 2 loss is: 0.00520021434161567\n",
      "Epoch 4 Batch 3 loss is: 0.006655417440726137\n",
      "Epoch 4 Batch 4 loss is: 0.007380588106580532\n",
      "Epoch 4 Batch 5 loss is: 0.0075192358855298855\n",
      "Epoch 4 Batch 6 loss is: 0.012403985741994425\n",
      "Epoch 4 Batch 7 loss is: 0.011379830857219773\n",
      "Epoch 4 Batch 8 loss is: 0.005870022484572814\n",
      "Epoch 4 Batch 9 loss is: 0.006683259099626279\n",
      "Epoch 4 Batch 10 loss is: 0.009347826402701228\n",
      "Epoch 4 Batch 11 loss is: 0.011093795824841936\n",
      "Epoch 4 Batch 12 loss is: 0.013088189186694283\n",
      "Epoch 4 Batch 13 loss is: 0.008413297762447888\n",
      "Epoch 4 Batch 14 loss is: 0.004207447944164263\n",
      "Epoch 4 Batch 15 loss is: 0.008346996428660868\n",
      "Epoch 4 Batch 16 loss is: 0.006965366402902309\n",
      "Epoch 4 Batch 17 loss is: 0.0029496351516103184\n",
      "Epoch 4 Batch 18 loss is: 0.0036045908414290297\n",
      "Epoch 4 Batch 19 loss is: 0.007982388224759683\n",
      "Epoch 4 Batch 20 loss is: 0.009128047030672413\n",
      "Epoch 4 Batch 21 loss is: 0.0050901917223040985\n",
      "Epoch 4 Batch 22 loss is: 0.0038668537396461746\n",
      "Epoch 4 Batch 23 loss is: 0.011799250800975969\n",
      "Epoch 4 Batch 24 loss is: 0.004321963022616811\n",
      "Epoch 4 Batch 25 loss is: 0.006490050686486057\n",
      "Epoch 4 Batch 26 loss is: 0.005851771052553971\n",
      "Epoch 4 Batch 27 loss is: 0.003990412811072907\n",
      "Epoch 4 Batch 28 loss is: 0.0038083498703935467\n",
      "Epoch 4 Batch 29 loss is: 0.008203462099991548\n",
      "Epoch 4 Batch 30 loss is: 0.011230125524460916\n",
      "Epoch 4 Batch 31 loss is: 0.005438711101320071\n",
      "Epoch 4 Batch 32 loss is: 0.0031599309734995154\n",
      "Epoch 4 Batch 33 loss is: 0.006218375698581321\n",
      "Epoch 4 Batch 34 loss is: 0.010501983332871773\n",
      "Epoch 4 Batch 35 loss is: 0.005513441460186061\n",
      "Epoch 4 Batch 36 loss is: 0.004684333353649054\n",
      "Epoch 4 Batch 37 loss is: 0.006592829357930986\n",
      "Epoch 4 Batch 38 loss is: 0.0073230816313523545\n",
      "Epoch 4 Batch 39 loss is: 0.005705152795535895\n",
      "Epoch 4 Batch 40 loss is: 0.0040827169658438104\n",
      "Epoch 4 Batch 41 loss is: 0.0043731953769054616\n",
      "Epoch 4 Batch 42 loss is: 0.005857524983394432\n",
      "Epoch 4 Batch 43 loss is: 0.006877426397893487\n",
      "Epoch 4 Batch 44 loss is: 0.006074263386485038\n",
      "Epoch 4 Batch 45 loss is: 0.0032695415436965903\n",
      "Epoch 4 Batch 46 loss is: 0.004885998207652022\n",
      "Epoch 4 Batch 47 loss is: 0.005336040527947799\n",
      "Epoch 4 Batch 48 loss is: 0.006056268959698331\n",
      "Epoch 4 Batch 49 loss is: 0.00501333524194159\n",
      "Epoch 4 Batch 50 loss is: 0.006393018874436187\n",
      "Epoch 4 Batch 51 loss is: 0.0067901553267113715\n",
      "Epoch 4 Batch 52 loss is: 0.0071205356680659904\n",
      "Epoch 4 Batch 53 loss is: 0.005846445939073692\n",
      "Epoch 4 Batch 54 loss is: 0.005980551176769756\n",
      "Epoch 4 Batch 55 loss is: 0.009957922937030392\n",
      "Epoch 4 Batch 56 loss is: 0.006270175629471737\n",
      "Epoch 4 Batch 57 loss is: 0.004093921157891896\n",
      "Epoch 4 Batch 58 loss is: 0.004055970743960925\n",
      "Epoch 4 Batch 59 loss is: 0.004483534079702191\n",
      "Epoch 4 Batch 60 loss is: 0.003500885114899172\n",
      "Epoch 4 Batch 61 loss is: 0.005354208236605717\n",
      "Epoch 4 Batch 62 loss is: 0.0034623786470381526\n",
      "Epoch 4 Batch 63 loss is: 0.005611824805563366\n",
      "Epoch 4 Batch 64 loss is: 0.007913235151603443\n",
      "Epoch 4 Batch 65 loss is: 0.001751833182335692\n",
      "Epoch 4 Batch 66 loss is: 0.007476524590255451\n",
      "Epoch 4 Batch 67 loss is: 0.0042560029728562895\n",
      "Epoch 4 Batch 68 loss is: 0.005513600242836922\n",
      "Epoch 4 Batch 69 loss is: 0.004605995347361554\n",
      "Epoch 4 Batch 70 loss is: 0.005125142767326807\n",
      "Epoch 4 Batch 71 loss is: 0.008047673603208084\n",
      "Epoch 4 Batch 72 loss is: 0.005726586746294669\n",
      "Epoch 4 Batch 73 loss is: 0.0071927975096705175\n",
      "Epoch 4 Batch 74 loss is: 0.008788748417139254\n",
      "Epoch 4 Batch 75 loss is: 0.005981070449996366\n",
      "Epoch 4 Batch 76 loss is: 0.01237731680049651\n",
      "Epoch 4 Batch 77 loss is: 0.005025071543199229\n",
      "Epoch 4 Batch 78 loss is: 0.007540330623063885\n",
      "Epoch 4 Batch 79 loss is: 0.006060542813192053\n",
      "Epoch 4 Batch 80 loss is: 0.0035157130058056296\n",
      "Epoch 4 Batch 81 loss is: 0.0064841871171007525\n",
      "Epoch 4 Batch 82 loss is: 0.006021611799909697\n",
      "Epoch 4 Batch 83 loss is: 0.005760133234979286\n",
      "Epoch 4 Batch 84 loss is: 0.006265135279934716\n",
      "Epoch 4 Batch 85 loss is: 0.004818545951966762\n",
      "Epoch 4 Batch 86 loss is: 0.005516964228660678\n",
      "Epoch 4 Batch 87 loss is: 0.005486558489432672\n",
      "Epoch 4 Batch 88 loss is: 0.00629965748882622\n",
      "Epoch 4 Batch 89 loss is: 0.009863561057165763\n",
      "Epoch 4 Batch 90 loss is: 0.007214909776008251\n",
      "Epoch 4 Batch 91 loss is: 0.004893433657459123\n",
      "Epoch 4 Batch 92 loss is: 0.00608530986800646\n",
      "Epoch 4 Batch 93 loss is: 0.003593171334907083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 5/30 [26:52<2:14:36, 323.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 trn loss is: 0.006331152021418543 val loss is: 0.1899711493104799\n",
      "Epoch 5 Batch 0 loss is: 0.00635381414230937\n",
      "Epoch 5 Batch 1 loss is: 0.002834773270445273\n",
      "Epoch 5 Batch 2 loss is: 0.005239060737044182\n",
      "Epoch 5 Batch 3 loss is: 0.003242006125278086\n",
      "Epoch 5 Batch 4 loss is: 0.005119372727966815\n",
      "Epoch 5 Batch 5 loss is: 0.004447210587629442\n",
      "Epoch 5 Batch 6 loss is: 0.0039348512104122576\n",
      "Epoch 5 Batch 7 loss is: 0.0024460410332303203\n",
      "Epoch 5 Batch 8 loss is: 0.004562013520453772\n",
      "Epoch 5 Batch 9 loss is: 0.004062582332145747\n",
      "Epoch 5 Batch 10 loss is: 0.004034481326151536\n",
      "Epoch 5 Batch 11 loss is: 0.005001651826457885\n",
      "Epoch 5 Batch 12 loss is: 0.004070209614732315\n",
      "Epoch 5 Batch 13 loss is: 0.005593114318720858\n",
      "Epoch 5 Batch 14 loss is: 0.005243853153786743\n",
      "Epoch 5 Batch 15 loss is: 0.004160651079607156\n",
      "Epoch 5 Batch 16 loss is: 0.0032511826021499484\n",
      "Epoch 5 Batch 17 loss is: 0.0033378072259752636\n",
      "Epoch 5 Batch 18 loss is: 0.003239877860187856\n",
      "Epoch 5 Batch 19 loss is: 0.0032755662881994376\n",
      "Epoch 5 Batch 20 loss is: 0.005561782426105388\n",
      "Epoch 5 Batch 21 loss is: 0.002289982093179338\n",
      "Epoch 5 Batch 22 loss is: 0.0029100966936313455\n",
      "Epoch 5 Batch 23 loss is: 0.0030178442892335984\n",
      "Epoch 5 Batch 24 loss is: 0.0034761982066126742\n",
      "Epoch 5 Batch 25 loss is: 0.003735748746583217\n",
      "Epoch 5 Batch 26 loss is: 0.002555692361649591\n",
      "Epoch 5 Batch 27 loss is: 0.005263351500994962\n",
      "Epoch 5 Batch 28 loss is: 0.0027199546409227555\n",
      "Epoch 5 Batch 29 loss is: 0.0034640222480972465\n",
      "Epoch 5 Batch 30 loss is: 0.003491159595532037\n",
      "Epoch 5 Batch 31 loss is: 0.004062206764383648\n",
      "Epoch 5 Batch 32 loss is: 0.002140289324387865\n",
      "Epoch 5 Batch 33 loss is: 0.002844166343569725\n",
      "Epoch 5 Batch 34 loss is: 0.007774930105410315\n",
      "Epoch 5 Batch 35 loss is: 0.004891483731959501\n",
      "Epoch 5 Batch 36 loss is: 0.004461767764842813\n",
      "Epoch 5 Batch 37 loss is: 0.003062423820802517\n",
      "Epoch 5 Batch 38 loss is: 0.00750326954998286\n",
      "Epoch 5 Batch 39 loss is: 0.00318248389115323\n",
      "Epoch 5 Batch 40 loss is: 0.0032476936687053618\n",
      "Epoch 5 Batch 41 loss is: 0.0033559245136657503\n",
      "Epoch 5 Batch 42 loss is: 0.0029094880111712484\n",
      "Epoch 5 Batch 43 loss is: 0.0028249417041135414\n",
      "Epoch 5 Batch 44 loss is: 0.003249663855991258\n",
      "Epoch 5 Batch 45 loss is: 0.0038970442865012465\n",
      "Epoch 5 Batch 46 loss is: 0.0029837824227605127\n",
      "Epoch 5 Batch 47 loss is: 0.0035139836273822074\n",
      "Epoch 5 Batch 48 loss is: 0.003139822924241464\n",
      "Epoch 5 Batch 49 loss is: 0.002027456945773404\n",
      "Epoch 5 Batch 50 loss is: 0.0040233454113552366\n",
      "Epoch 5 Batch 51 loss is: 0.005158066788240312\n",
      "Epoch 5 Batch 52 loss is: 0.0025226641517843026\n",
      "Epoch 5 Batch 53 loss is: 0.0032514131546089093\n",
      "Epoch 5 Batch 54 loss is: 0.0028703147440393905\n",
      "Epoch 5 Batch 55 loss is: 0.002894213368811194\n",
      "Epoch 5 Batch 56 loss is: 0.003504487344993663\n",
      "Epoch 5 Batch 57 loss is: 0.004784013355125888\n",
      "Epoch 5 Batch 58 loss is: 0.0016753121523228269\n",
      "Epoch 5 Batch 59 loss is: 0.0039289223619121336\n",
      "Epoch 5 Batch 60 loss is: 0.005271672232847019\n",
      "Epoch 5 Batch 61 loss is: 0.00203843398559151\n",
      "Epoch 5 Batch 62 loss is: 0.0035998101417454246\n",
      "Epoch 5 Batch 63 loss is: 0.0037105126213619025\n",
      "Epoch 5 Batch 64 loss is: 0.0027351240658923834\n",
      "Epoch 5 Batch 65 loss is: 0.0034114326559970464\n",
      "Epoch 5 Batch 66 loss is: 0.003730943466190292\n",
      "Epoch 5 Batch 67 loss is: 0.004365836576612842\n",
      "Epoch 5 Batch 68 loss is: 0.0035476341971149832\n",
      "Epoch 5 Batch 69 loss is: 0.0028383243489543256\n",
      "Epoch 5 Batch 70 loss is: 0.003088750694223255\n",
      "Epoch 5 Batch 71 loss is: 0.0022107962800658144\n",
      "Epoch 5 Batch 72 loss is: 0.0036707135317169087\n",
      "Epoch 5 Batch 73 loss is: 0.005108238685670064\n",
      "Epoch 5 Batch 74 loss is: 0.002665046385004016\n",
      "Epoch 5 Batch 75 loss is: 0.0057438760153062296\n",
      "Epoch 5 Batch 76 loss is: 0.0033241513170545528\n",
      "Epoch 5 Batch 77 loss is: 0.0033727413529637575\n",
      "Epoch 5 Batch 78 loss is: 0.004645590134253321\n",
      "Epoch 5 Batch 79 loss is: 0.0027550012475399655\n",
      "Epoch 5 Batch 80 loss is: 0.003970131725443314\n",
      "Epoch 5 Batch 81 loss is: 0.0035724832958246465\n",
      "Epoch 5 Batch 82 loss is: 0.004249365737149991\n",
      "Epoch 5 Batch 83 loss is: 0.0025097141316858776\n",
      "Epoch 5 Batch 84 loss is: 0.0047203072331592705\n",
      "Epoch 5 Batch 85 loss is: 0.0030268464928013116\n",
      "Epoch 5 Batch 86 loss is: 0.0021451828254203777\n",
      "Epoch 5 Batch 87 loss is: 0.004115980366194213\n",
      "Epoch 5 Batch 88 loss is: 0.002554082859848303\n",
      "Epoch 5 Batch 89 loss is: 0.003165277764746127\n",
      "Epoch 5 Batch 90 loss is: 0.002421102841548593\n",
      "Epoch 5 Batch 91 loss is: 0.002381518117093364\n",
      "Epoch 5 Batch 92 loss is: 0.00510495081495435\n",
      "Epoch 5 Batch 93 loss is: 0.0044601772449833115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 6/30 [32:19<2:09:39, 324.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 trn loss is: 0.003700566821940186 val loss is: 0.18695301702435443\n",
      "Epoch 6 Batch 0 loss is: 0.002378847697910871\n",
      "Epoch 6 Batch 1 loss is: 0.0016280277403948418\n",
      "Epoch 6 Batch 2 loss is: 0.0012248494810085475\n",
      "Epoch 6 Batch 3 loss is: 0.0025620186900371066\n",
      "Epoch 6 Batch 4 loss is: 0.0028393442876458153\n",
      "Epoch 6 Batch 5 loss is: 0.0010323838557442617\n",
      "Epoch 6 Batch 6 loss is: 0.003096869538903183\n",
      "Epoch 6 Batch 7 loss is: 0.0023976359398107405\n",
      "Epoch 6 Batch 8 loss is: 0.0011345827989219734\n",
      "Epoch 6 Batch 9 loss is: 0.001962797044596248\n",
      "Epoch 6 Batch 10 loss is: 0.0027706179672060097\n",
      "Epoch 6 Batch 11 loss is: 0.002286142821119353\n",
      "Epoch 6 Batch 12 loss is: 0.0017167877877762312\n",
      "Epoch 6 Batch 13 loss is: 0.003046461144250685\n",
      "Epoch 6 Batch 14 loss is: 0.002957660515465387\n",
      "Epoch 6 Batch 15 loss is: 0.0027636322748466097\n",
      "Epoch 6 Batch 16 loss is: 0.0024795895527892496\n",
      "Epoch 6 Batch 17 loss is: 0.00246559974017084\n",
      "Epoch 6 Batch 18 loss is: 0.0034293549484012923\n",
      "Epoch 6 Batch 19 loss is: 0.0025081077709004046\n",
      "Epoch 6 Batch 20 loss is: 0.002091621993210566\n",
      "Epoch 6 Batch 21 loss is: 0.0026680918181154214\n",
      "Epoch 6 Batch 22 loss is: 0.003199543600363531\n",
      "Epoch 6 Batch 23 loss is: 0.0041633462035211245\n",
      "Epoch 6 Batch 24 loss is: 0.0028729384937390633\n",
      "Epoch 6 Batch 25 loss is: 0.0024233704160890567\n",
      "Epoch 6 Batch 26 loss is: 0.003943113181263793\n",
      "Epoch 6 Batch 27 loss is: 0.0032298015218412248\n",
      "Epoch 6 Batch 28 loss is: 0.0041715202209253255\n",
      "Epoch 6 Batch 29 loss is: 0.003607205758080501\n",
      "Epoch 6 Batch 30 loss is: 0.002045756894067286\n",
      "Epoch 6 Batch 31 loss is: 0.0022726040739200924\n",
      "Epoch 6 Batch 32 loss is: 0.00340749588532276\n",
      "Epoch 6 Batch 33 loss is: 0.001316769031939297\n",
      "Epoch 6 Batch 34 loss is: 0.002286348997696924\n",
      "Epoch 6 Batch 35 loss is: 0.0025246859951946732\n",
      "Epoch 6 Batch 36 loss is: 0.001883626678290753\n",
      "Epoch 6 Batch 37 loss is: 0.0018626820390929073\n",
      "Epoch 6 Batch 38 loss is: 0.0022750244725971433\n",
      "Epoch 6 Batch 39 loss is: 0.0019417415232290717\n",
      "Epoch 6 Batch 40 loss is: 0.002800567076539906\n",
      "Epoch 6 Batch 41 loss is: 0.0018578933220975812\n",
      "Epoch 6 Batch 42 loss is: 0.0034512395322288587\n",
      "Epoch 6 Batch 43 loss is: 0.002212909240389275\n",
      "Epoch 6 Batch 44 loss is: 0.004259710548331839\n",
      "Epoch 6 Batch 45 loss is: 0.002801800150650043\n",
      "Epoch 6 Batch 46 loss is: 0.0016708138522083023\n",
      "Epoch 6 Batch 47 loss is: 0.006180444226142162\n",
      "Epoch 6 Batch 48 loss is: 0.0013415308853310629\n",
      "Epoch 6 Batch 49 loss is: 0.002771339399173461\n",
      "Epoch 6 Batch 50 loss is: 0.0014791887798529283\n",
      "Epoch 6 Batch 51 loss is: 0.0020605354864978885\n",
      "Epoch 6 Batch 52 loss is: 0.0036376661869745182\n",
      "Epoch 6 Batch 53 loss is: 0.0024700704797671474\n",
      "Epoch 6 Batch 54 loss is: 0.002703106892128891\n",
      "Epoch 6 Batch 55 loss is: 0.002493804343859741\n",
      "Epoch 6 Batch 56 loss is: 0.004387530050135915\n",
      "Epoch 6 Batch 57 loss is: 0.003491662789662821\n",
      "Epoch 6 Batch 58 loss is: 0.0017398957631420586\n",
      "Epoch 6 Batch 59 loss is: 0.0018610659005500216\n",
      "Epoch 6 Batch 60 loss is: 0.002037066543815271\n",
      "Epoch 6 Batch 61 loss is: 0.0015341533591723315\n",
      "Epoch 6 Batch 62 loss is: 0.0032933776872893124\n",
      "Epoch 6 Batch 63 loss is: 0.0025819726965908294\n",
      "Epoch 6 Batch 64 loss is: 0.0017941785752470215\n",
      "Epoch 6 Batch 65 loss is: 0.0038201750839289635\n",
      "Epoch 6 Batch 66 loss is: 0.0020688361196541057\n",
      "Epoch 6 Batch 67 loss is: 0.0016657500522987098\n",
      "Epoch 6 Batch 68 loss is: 0.002022191224865798\n",
      "Epoch 6 Batch 69 loss is: 0.002411865754333794\n",
      "Epoch 6 Batch 70 loss is: 0.0021620169975529535\n",
      "Epoch 6 Batch 71 loss is: 0.0012989012287290553\n",
      "Epoch 6 Batch 72 loss is: 0.0026197877140614876\n",
      "Epoch 6 Batch 73 loss is: 0.0021344695130200127\n",
      "Epoch 6 Batch 74 loss is: 0.0027251832401293777\n",
      "Epoch 6 Batch 75 loss is: 0.0011551885257993178\n",
      "Epoch 6 Batch 76 loss is: 0.0037958109512324493\n",
      "Epoch 6 Batch 77 loss is: 0.0023123740626366167\n",
      "Epoch 6 Batch 78 loss is: 0.001810999512514068\n",
      "Epoch 6 Batch 79 loss is: 0.0027792634879543715\n",
      "Epoch 6 Batch 80 loss is: 0.0018970283787359203\n",
      "Epoch 6 Batch 81 loss is: 0.002526065904437118\n",
      "Epoch 6 Batch 82 loss is: 0.001740266136367102\n",
      "Epoch 6 Batch 83 loss is: 0.0017224204984031626\n",
      "Epoch 6 Batch 84 loss is: 0.0019636184326489096\n",
      "Epoch 6 Batch 85 loss is: 0.002578076890178664\n",
      "Epoch 6 Batch 86 loss is: 0.0022045337713586357\n",
      "Epoch 6 Batch 87 loss is: 0.003885474185279326\n",
      "Epoch 6 Batch 88 loss is: 0.0018144304705381132\n",
      "Epoch 6 Batch 89 loss is: 0.0021276129496454626\n",
      "Epoch 6 Batch 90 loss is: 0.002970652343341366\n",
      "Epoch 6 Batch 91 loss is: 0.0013871847516490731\n",
      "Epoch 6 Batch 92 loss is: 0.0020542378440342665\n",
      "Epoch 6 Batch 93 loss is: 0.004091331033775146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [37:39<2:03:49, 323.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 trn loss is: 0.002505615651311519 val loss is: 0.17494964313679628\n",
      "Epoch 7 Batch 0 loss is: 0.0016710678263526428\n",
      "Epoch 7 Batch 1 loss is: 0.0016931128034728478\n",
      "Epoch 7 Batch 2 loss is: 0.001880093763787277\n",
      "Epoch 7 Batch 3 loss is: 0.0013338757203382556\n",
      "Epoch 7 Batch 4 loss is: 0.001788059738387607\n",
      "Epoch 7 Batch 5 loss is: 0.0018304121531627971\n",
      "Epoch 7 Batch 6 loss is: 0.0022303210504240074\n",
      "Epoch 7 Batch 7 loss is: 0.0017149370816303626\n",
      "Epoch 7 Batch 8 loss is: 0.0015026987081997116\n",
      "Epoch 7 Batch 9 loss is: 0.002262583247986309\n",
      "Epoch 7 Batch 10 loss is: 0.002272649436851519\n",
      "Epoch 7 Batch 11 loss is: 0.0018087179108458192\n",
      "Epoch 7 Batch 12 loss is: 0.00229387898486929\n",
      "Epoch 7 Batch 13 loss is: 0.001192747613273113\n",
      "Epoch 7 Batch 14 loss is: 0.0014062016522378448\n",
      "Epoch 7 Batch 15 loss is: 0.0009763709348558554\n",
      "Epoch 7 Batch 16 loss is: 0.0021795494651941044\n",
      "Epoch 7 Batch 17 loss is: 0.0019270039762862012\n",
      "Epoch 7 Batch 18 loss is: 0.0022152327139485094\n",
      "Epoch 7 Batch 19 loss is: 0.0028393493831403304\n",
      "Epoch 7 Batch 20 loss is: 0.0021199564564026475\n",
      "Epoch 7 Batch 21 loss is: 0.0016279649964428937\n",
      "Epoch 7 Batch 22 loss is: 0.0028547389260329225\n",
      "Epoch 7 Batch 23 loss is: 0.0013180451369420136\n",
      "Epoch 7 Batch 24 loss is: 0.001684469214974674\n",
      "Epoch 7 Batch 25 loss is: 0.0007562637209803569\n",
      "Epoch 7 Batch 26 loss is: 0.001979807356920181\n",
      "Epoch 7 Batch 27 loss is: 0.002070602372510327\n",
      "Epoch 7 Batch 28 loss is: 0.0018706919459746275\n",
      "Epoch 7 Batch 29 loss is: 0.0021381045687081015\n",
      "Epoch 7 Batch 30 loss is: 0.0017803358521395532\n",
      "Epoch 7 Batch 31 loss is: 0.0012932231138470958\n",
      "Epoch 7 Batch 32 loss is: 0.0027344938202882928\n",
      "Epoch 7 Batch 33 loss is: 0.0013942625434205998\n",
      "Epoch 7 Batch 34 loss is: 0.0020990145226746173\n",
      "Epoch 7 Batch 35 loss is: 0.0020874800666542963\n",
      "Epoch 7 Batch 36 loss is: 0.0011936096655667684\n",
      "Epoch 7 Batch 37 loss is: 0.0020126695877801617\n",
      "Epoch 7 Batch 38 loss is: 0.0017483291615164613\n",
      "Epoch 7 Batch 39 loss is: 0.0017655586772639727\n",
      "Epoch 7 Batch 40 loss is: 0.002394429085702067\n",
      "Epoch 7 Batch 41 loss is: 0.0024802271957467545\n",
      "Epoch 7 Batch 42 loss is: 0.001851232740256563\n",
      "Epoch 7 Batch 43 loss is: 0.0018918518388168811\n",
      "Epoch 7 Batch 44 loss is: 0.0011088903181714945\n",
      "Epoch 7 Batch 45 loss is: 0.00173676514712722\n",
      "Epoch 7 Batch 46 loss is: 0.002574405627963022\n",
      "Epoch 7 Batch 47 loss is: 0.001433774235999863\n",
      "Epoch 7 Batch 48 loss is: 0.002321705510475738\n",
      "Epoch 7 Batch 49 loss is: 0.0015155498474962077\n",
      "Epoch 7 Batch 50 loss is: 0.0015184555704830652\n",
      "Epoch 7 Batch 51 loss is: 0.0014302010842640793\n",
      "Epoch 7 Batch 52 loss is: 0.0013501290646169887\n",
      "Epoch 7 Batch 53 loss is: 0.001770394237410938\n",
      "Epoch 7 Batch 54 loss is: 0.0010495256551987353\n",
      "Epoch 7 Batch 55 loss is: 0.001261524274533746\n",
      "Epoch 7 Batch 56 loss is: 0.0016213968035462282\n",
      "Epoch 7 Batch 57 loss is: 0.0016056511302105037\n",
      "Epoch 7 Batch 58 loss is: 0.0027359936683910985\n",
      "Epoch 7 Batch 59 loss is: 0.0020506064462457846\n",
      "Epoch 7 Batch 60 loss is: 0.00099088374059446\n",
      "Epoch 7 Batch 61 loss is: 0.0013694010086080747\n",
      "Epoch 7 Batch 62 loss is: 0.0013534957693455852\n",
      "Epoch 7 Batch 63 loss is: 0.001553443150420056\n",
      "Epoch 7 Batch 64 loss is: 0.0016396912055178584\n",
      "Epoch 7 Batch 65 loss is: 0.002108083003160246\n",
      "Epoch 7 Batch 66 loss is: 0.002103850965922973\n",
      "Epoch 7 Batch 67 loss is: 0.0011786427162485088\n",
      "Epoch 7 Batch 68 loss is: 0.001944182954732317\n",
      "Epoch 7 Batch 69 loss is: 0.001605739981078429\n",
      "Epoch 7 Batch 70 loss is: 0.0014185812544335855\n",
      "Epoch 7 Batch 71 loss is: 0.0021378125306976753\n",
      "Epoch 7 Batch 72 loss is: 0.0025816154587174605\n",
      "Epoch 7 Batch 73 loss is: 0.0018311905365667514\n",
      "Epoch 7 Batch 74 loss is: 0.0010780808766601524\n",
      "Epoch 7 Batch 75 loss is: 0.0022899710595207525\n",
      "Epoch 7 Batch 76 loss is: 0.0011539956161317614\n",
      "Epoch 7 Batch 77 loss is: 0.0016184904616630765\n",
      "Epoch 7 Batch 78 loss is: 0.0023565344648671526\n",
      "Epoch 7 Batch 79 loss is: 0.0019222579583755698\n",
      "Epoch 7 Batch 80 loss is: 0.0019762437756821547\n",
      "Epoch 7 Batch 81 loss is: 0.002192811131101067\n",
      "Epoch 7 Batch 82 loss is: 0.0014300441469794834\n",
      "Epoch 7 Batch 83 loss is: 0.0015773503966639642\n",
      "Epoch 7 Batch 84 loss is: 0.0018344443765670113\n",
      "Epoch 7 Batch 85 loss is: 0.0016482933147099743\n",
      "Epoch 7 Batch 86 loss is: 0.0016650523128130127\n",
      "Epoch 7 Batch 87 loss is: 0.0021950798411299152\n",
      "Epoch 7 Batch 88 loss is: 0.0027745738857229557\n",
      "Epoch 7 Batch 89 loss is: 0.0025395344499481356\n",
      "Epoch 7 Batch 90 loss is: 0.0024952831320932253\n",
      "Epoch 7 Batch 91 loss is: 0.0025681782413095296\n",
      "Epoch 7 Batch 92 loss is: 0.0014000039105055131\n",
      "Epoch 7 Batch 93 loss is: 0.00208449043340509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [43:05<1:58:42, 323.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 trn loss is: 0.0018283885469876097 val loss is: 0.19116536693314778\n",
      "Epoch 8 Batch 0 loss is: 0.0024529295023719417\n",
      "Epoch 8 Batch 1 loss is: 0.000998507855704034\n",
      "Epoch 8 Batch 2 loss is: 0.001994420653015965\n",
      "Epoch 8 Batch 3 loss is: 0.0013572493091011494\n",
      "Epoch 8 Batch 4 loss is: 0.0009859615288616889\n",
      "Epoch 8 Batch 5 loss is: 0.0017456708916885806\n",
      "Epoch 8 Batch 6 loss is: 0.0014104079463032803\n",
      "Epoch 8 Batch 7 loss is: 0.0019910868094938296\n",
      "Epoch 8 Batch 8 loss is: 0.0011474811880551764\n",
      "Epoch 8 Batch 9 loss is: 0.0016603986408588157\n",
      "Epoch 8 Batch 10 loss is: 0.001383618895809633\n",
      "Epoch 8 Batch 11 loss is: 0.001157685591228533\n",
      "Epoch 8 Batch 12 loss is: 0.0017452896580850564\n",
      "Epoch 8 Batch 13 loss is: 0.0006777849095801732\n",
      "Epoch 8 Batch 14 loss is: 0.0012077451474695996\n",
      "Epoch 8 Batch 15 loss is: 0.0029785159771874704\n",
      "Epoch 8 Batch 16 loss is: 0.0015395220936051145\n",
      "Epoch 8 Batch 17 loss is: 0.0008971635313396576\n",
      "Epoch 8 Batch 18 loss is: 0.0016173423604866363\n",
      "Epoch 8 Batch 19 loss is: 0.0016036579678349483\n",
      "Epoch 8 Batch 20 loss is: 0.001183361537058308\n",
      "Epoch 8 Batch 21 loss is: 0.002057867448127458\n",
      "Epoch 8 Batch 22 loss is: 0.0008112151463001045\n",
      "Epoch 8 Batch 23 loss is: 0.0013231408150272727\n",
      "Epoch 8 Batch 24 loss is: 0.0016276860175733058\n",
      "Epoch 8 Batch 25 loss is: 0.0017155349514732876\n",
      "Epoch 8 Batch 26 loss is: 0.0013818959529605715\n",
      "Epoch 8 Batch 27 loss is: 0.0010738588945835186\n",
      "Epoch 8 Batch 28 loss is: 0.0013164409035849426\n",
      "Epoch 8 Batch 29 loss is: 0.001156415461592862\n",
      "Epoch 8 Batch 30 loss is: 0.0017544490703450559\n",
      "Epoch 8 Batch 31 loss is: 0.0009967184425553437\n",
      "Epoch 8 Batch 32 loss is: 0.0017126734492735806\n",
      "Epoch 8 Batch 33 loss is: 0.0011785164980952583\n",
      "Epoch 8 Batch 34 loss is: 0.0013648520783894587\n",
      "Epoch 8 Batch 35 loss is: 0.0017255930560670406\n",
      "Epoch 8 Batch 36 loss is: 0.0009288152091063277\n",
      "Epoch 8 Batch 37 loss is: 0.0012075940009085191\n",
      "Epoch 8 Batch 38 loss is: 0.0013482959074290572\n",
      "Epoch 8 Batch 39 loss is: 0.0011673176741177826\n",
      "Epoch 8 Batch 40 loss is: 0.0015171791198680276\n",
      "Epoch 8 Batch 41 loss is: 0.001986025808348266\n",
      "Epoch 8 Batch 42 loss is: 0.0014307694324938798\n",
      "Epoch 8 Batch 43 loss is: 0.0018186522473754962\n",
      "Epoch 8 Batch 44 loss is: 0.0010661943134128648\n",
      "Epoch 8 Batch 45 loss is: 0.0010352504790838424\n",
      "Epoch 8 Batch 46 loss is: 0.0017252513113861904\n",
      "Epoch 8 Batch 47 loss is: 0.0014820897681950384\n",
      "Epoch 8 Batch 48 loss is: 0.0011206276497442503\n",
      "Epoch 8 Batch 49 loss is: 0.001337956537123238\n",
      "Epoch 8 Batch 50 loss is: 0.0013960828427483326\n",
      "Epoch 8 Batch 51 loss is: 0.001539468534202489\n",
      "Epoch 8 Batch 52 loss is: 0.001912642152318907\n",
      "Epoch 8 Batch 53 loss is: 0.0014693499017515185\n",
      "Epoch 8 Batch 54 loss is: 0.0018288983741626907\n",
      "Epoch 8 Batch 55 loss is: 0.0014698925580340946\n",
      "Epoch 8 Batch 56 loss is: 0.0020073689140826783\n",
      "Epoch 8 Batch 57 loss is: 0.0017391624911490074\n",
      "Epoch 8 Batch 58 loss is: 0.001276476618710447\n",
      "Epoch 8 Batch 59 loss is: 0.0007632862107025081\n",
      "Epoch 8 Batch 60 loss is: 0.0013942240100048291\n",
      "Epoch 8 Batch 61 loss is: 0.0008706375832469603\n",
      "Epoch 8 Batch 62 loss is: 0.001337967429964948\n",
      "Epoch 8 Batch 63 loss is: 0.0009861692364251933\n",
      "Epoch 8 Batch 64 loss is: 0.0016638066154948206\n",
      "Epoch 8 Batch 65 loss is: 0.001498369918754688\n",
      "Epoch 8 Batch 66 loss is: 0.0011230701198455032\n",
      "Epoch 8 Batch 67 loss is: 0.002385925823952585\n",
      "Epoch 8 Batch 68 loss is: 0.0009164892506710487\n",
      "Epoch 8 Batch 69 loss is: 0.0015186577382344524\n",
      "Epoch 8 Batch 70 loss is: 0.0012356946479724702\n",
      "Epoch 8 Batch 71 loss is: 0.0007912475948353167\n",
      "Epoch 8 Batch 72 loss is: 0.0014557210779573401\n",
      "Epoch 8 Batch 73 loss is: 0.0015127587107554774\n",
      "Epoch 8 Batch 74 loss is: 0.001206697989575171\n",
      "Epoch 8 Batch 75 loss is: 0.0011298008771373229\n",
      "Epoch 8 Batch 76 loss is: 0.0013791410927399758\n",
      "Epoch 8 Batch 77 loss is: 0.0021679835299710957\n",
      "Epoch 8 Batch 78 loss is: 0.0008916104253983074\n",
      "Epoch 8 Batch 79 loss is: 0.0010234852922496648\n",
      "Epoch 8 Batch 80 loss is: 0.0011313045910737902\n",
      "Epoch 8 Batch 81 loss is: 0.0015399636981357646\n",
      "Epoch 8 Batch 82 loss is: 0.0014192125505487023\n",
      "Epoch 8 Batch 83 loss is: 0.0010374978061045445\n",
      "Epoch 8 Batch 84 loss is: 0.0015557654332587609\n",
      "Epoch 8 Batch 85 loss is: 0.0009483561970210038\n",
      "Epoch 8 Batch 86 loss is: 0.0012284909649927893\n",
      "Epoch 8 Batch 87 loss is: 0.0012383969922152478\n",
      "Epoch 8 Batch 88 loss is: 0.0020653486715481034\n",
      "Epoch 8 Batch 89 loss is: 0.0014410877020765867\n",
      "Epoch 8 Batch 90 loss is: 0.0014645531014862692\n",
      "Epoch 8 Batch 91 loss is: 0.001208473027320262\n",
      "Epoch 8 Batch 92 loss is: 0.0008220560304899748\n",
      "Epoch 8 Batch 93 loss is: 0.0005446249138901856\n",
      "epoch 8 trn loss is: 0.001400445732796737 val loss is: 0.19575760383190507\n",
      "Early Stopping - losses going up\n"
     ]
    }
   ],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "weights = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "        for i, samples in enumerate(dataloader_train):\n",
    "            #Data\n",
    "            images = samples['data'].to(device).requires_grad_()\n",
    "            images = torch.unsqueeze(images,1)\n",
    "            labels = samples['label'].to(device)\n",
    "            _, labels = torch.max(labels,1) #converts from one hot to integer\n",
    "            \n",
    "            #Forward Pass\n",
    "            outputs = net(images)\n",
    "\n",
    "            #Backward Pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.data\n",
    "            print(\"Epoch {} Batch {} loss is: {}\".format(epoch, i, loss.data))\n",
    "            \n",
    "        epoch_training_loss = running_loss.item()/(i+1) #i + 1 since index starts from 0\n",
    "        \n",
    "        for j, val_samples in enumerate(dataloader_val):\n",
    "            #validation loss\n",
    "            images = val_samples['data'].to(device)\n",
    "            images = torch.unsqueeze(images,1)\n",
    "            labels = val_samples['label'].to(device)\n",
    "            _, labels = torch.max(labels,1)\n",
    "\n",
    "            #Forward Pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.data\n",
    "            \n",
    "        epoch_validation_loss = running_val_loss.item()/(j+1) #i + 1 since index starts from 0\n",
    "        \n",
    "        training_losses.append(epoch_training_loss)\n",
    "        validation_losses.append(epoch_validation_loss)\n",
    "        print('epoch', epoch, 'trn loss is:', epoch_training_loss, 'val loss is:', epoch_validation_loss)\n",
    "        \n",
    "        weights.append(net.state_dict())\n",
    "        if epoch >= 3:\n",
    "            #keep only the 3 most current network weights\n",
    "            del weights[0]\n",
    "            \n",
    "        \n",
    "        #Criteria for early stopping - if validation loss goes up after 3 iterations or stops changing over 3 epochs\n",
    "        if (epoch >= 3) and (abs(validation_losses[epoch] - validation_losses[epoch-1]) <= 0.0001) and (abs(validation_losses[epoch - 1] - validation_losses[epoch - 2]) <= 0.0001):\n",
    "            print(\"Early Stopping - losses stopped changing\")\n",
    "            for i in range(len(weights)):\n",
    "                path = \"CV_file/es_weights/weights\"+str(i)+'.pt'\n",
    "                torch.save(weights[i], path)\n",
    "            break   \n",
    "       \n",
    "        elif (epoch>=3) and (validation_losses[epoch]-validation_losses[epoch-1]>0) and (validation_losses[epoch-1]-validation_losses[epoch-2]>0):\n",
    "            print(\"Early Stopping - losses going up\")\n",
    "            for i in range(len(weights)):\n",
    "                path = \"CV_file/es_weights/weights\"+str(i)+'.pt'\n",
    "                torch.save(weights[i], path)\n",
    "            break\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These results are using the weights that it exited on (validation loss went up twice)\n",
    "dataloader_test = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=4) #0.9510791366906475 -> using two steps before 0.951558752997602\n",
    "dataloader_fulltrain = DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=4)#0.9922602491232313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop2D): Dropout2d(p=0.25)\n",
       "  (vp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=36864, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net = torch_classes.Net(num_of_classes=33)\n",
    "test_net.load_state_dict(torch.load('CV_file/es_weights/weights0.pt'))\n",
    "test_net.to(device)\n",
    "test_net.double()\n",
    "test_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array([])\n",
    "labels_predicted = np.array([])\n",
    "for ii, test_sample in enumerate(dataloader_test):\n",
    "    #Data\n",
    "    images = test_sample['data'].to(device)\n",
    "    images = torch.unsqueeze(images,1)\n",
    "    labels = test_sample['label'].to(device)\n",
    "    _, labels = torch.max(labels,1)\n",
    "    \n",
    "    #Forward pass\n",
    "    outputs = test_net(images)\n",
    "    \n",
    "    #Label classified\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    true_labels = np.append(true_labels, labels.cpu().numpy().astype('int8'))\n",
    "    labels_predicted = np.append(labels_predicted, predicted.cpu().numpy().astype('int8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951558752997602\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(true_labels==labels_predicted)\n",
    "print(accuracy/len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
